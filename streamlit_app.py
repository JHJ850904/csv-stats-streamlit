# streamlit_app.py
# ============================================
# ì‹¤í–‰:  python -m streamlit run streamlit_app.py
# í•„ìš” íŒ¨í‚¤ì§€:
#   pip install streamlit pandas numpy scipy scikit-learn statsmodels plotly python-dateutil
# ============================================

from __future__ import annotations
import textwrap
from dataclasses import dataclass
from typing import List, Optional, Tuple

import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px

from dateutil.parser import parse as dt_parse
from scipy import stats

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    roc_curve,
    accuracy_score,
    precision_recall_fscore_support,
    mean_squared_error,
    r2_score,
)
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.inspection import permutation_importance
from sklearn.ensemble import IsolationForest, RandomForestRegressor, RandomForestClassifier
from sklearn.impute import SimpleImputer

import statsmodels.api as sm
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from statsmodels.stats.outliers_influence import variance_inflation_factor

st.set_page_config(
    page_title="CSV í†µê³„ë¶„ì„ ì›¹ì•± (ë² íƒ€)", 
    page_icon="ğŸ“Š", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# =============================================================
#  ìœ í‹¸: íƒ€ì… ì¶”ë¡ 
# =============================================================
@dataclass
class InferredTypes:
    numeric: List[str]
    categorical: List[str]
    boolean: List[str]
    datetime: List[str]


def _looks_datetime(series: pd.Series, sample: int = 50) -> bool:
    if series.dtype.kind in ("M",):
        return True
    if series.dtype == "object":
        ex = series.dropna().astype(str).head(sample)
        ok = 0
        for v in ex:
            try:
                _ = dt_parse(v, fuzzy=False)
                ok += 1
            except Exception:
                pass
        return ok >= max(3, len(ex) // 2)
    return False


def infer_types(df: pd.DataFrame) -> InferredTypes:
    numeric: List[str] = []
    categorical: List[str] = []
    boolean: List[str] = []
    datetime_cols: List[str] = []

    for c in df.columns:
        s = df[c]
        if s.dropna().isin([0, 1, True, False, "0", "1", "true", "false"]).all():
            boolean.append(c)
            continue
        if _looks_datetime(s):
            datetime_cols.append(c)
            continue
        if pd.api.types.is_numeric_dtype(s):
            nunique = s.nunique(dropna=True)
            if nunique <= max(10, int(0.02 * len(s))):
                categorical.append(c)
            else:
                numeric.append(c)
        else:
            categorical.append(c)
    return InferredTypes(numeric=numeric, categorical=categorical, boolean=boolean, datetime=datetime_cols)

# =============================================================
#  íš¨ê³¼í¬ê¸° & í•´ì„ ìœ í‹¸
# =============================================================

def cohen_d(x: np.ndarray, y: np.ndarray) -> float:
    x = np.asarray(x, dtype=float)
    y = np.asarray(y, dtype=float)
    nx, ny = len(x), len(y)
    if nx < 2 or ny < 2:
        return np.nan
    vx = x.var(ddof=1)
    vy = y.var(ddof=1)
    s = np.sqrt(((nx - 1) * vx + (ny - 1) * vy) / (nx + ny - 2))
    return (x.mean() - y.mean()) / s if s != 0 else 0.0


def eta_squared_anova(groups: List[np.ndarray]) -> float:
    all_values = np.concatenate(groups)
    grand_mean = all_values.mean()
    ssb = sum(len(g) * (g.mean() - grand_mean) ** 2 for g in groups)
    sst = ((all_values - grand_mean) ** 2).sum()
    return float(ssb / sst) if sst > 0 else np.nan


def cramers_v(chi2: float, n: int, r: int, c: int) -> float:
    return float(np.sqrt(chi2 / (n * (min(r - 1, c - 1))))) if min(r, c) > 1 else np.nan


def _sig_text(p: float) -> str:
    return "ìœ ì˜í•¨ (p<0.05)" if (p is not None and p < 0.05) else "ìœ ì˜í•˜ì§€ ì•ŠìŒ (pâ‰¥0.05)"


def _cohen_d_level(d: float) -> str:
    if d is None or not np.isfinite(d):
        return "â€“"
    ad = abs(d)
    return "ë§¤ìš° ì‘ìŒ" if ad < 0.2 else ("ì‘ìŒ" if ad < 0.5 else ("ì¤‘ê°„" if ad < 0.8 else "í¼"))


def _eta2_level(e: float) -> str:
    if e is None or not np.isfinite(e):
        return "â€“"
    return "ë§¤ìš° ì‘ìŒ" if e < 0.01 else ("ì‘ìŒ" if e < 0.06 else ("ì¤‘ê°„" if e < 0.14 else "í¼"))


def _cramers_v_level(v: float) -> str:
    if v is None or not np.isfinite(v):
        return "â€“"
    av = abs(v)
    return "ì•½í•¨" if av < 0.1 else ("ë³´í†µ" if av < 0.3 else ("ê°•í•¨" if av < 0.5 else "ë§¤ìš° ê°•í•¨"))


def _r2_level(r2: float) -> str:
    if r2 is None or not np.isfinite(r2):
        return "â€“"
    return "ì•½í•¨" if r2 < 0.3 else ("ë³´í†µ" if r2 < 0.5 else ("ê°•í•¨" if r2 < 0.7 else "ë§¤ìš° ê°•í•¨"))


def _auc_level(auc: float) -> str:
    if auc is None or not np.isfinite(auc):
        return "â€“"
    return "ë¬´ì‘ìœ„ì— ê°€ê¹Œì›€" if auc < 0.6 else ("ë‚®ìŒ" if auc < 0.7 else ("ë³´í†µ" if auc < 0.8 else ("ì¢‹ìŒ" if auc < 0.9 else "ë§¤ìš° ì¢‹ìŒ")))

# =============================================================
#  ì‹œê°ì  í•´ì„ ë„êµ¬
# =============================================================

def create_significance_badge(p_value: float) -> str:
    """í†µê³„ì  ìœ ì˜ì„± ë°°ì§€ ìƒì„±"""
    if p_value < 0.001:
        return "ğŸŸ© **ë§¤ìš° ìœ ì˜í•¨** (p<0.001)"
    elif p_value < 0.01:
        return "ğŸŸ¢ **ìœ ì˜í•¨** (p<0.01)"
    elif p_value < 0.05:
        return "ğŸŸ¡ **ìœ ì˜í•¨** (p<0.05)"
    elif p_value < 0.1:
        return "ğŸŸ  **ê²½ê³„ì ** (p<0.1)"
    else:
        return "ğŸ”´ **ìœ ì˜í•˜ì§€ ì•ŠìŒ** (pâ‰¥0.1)"

def create_effect_size_badge(effect_size: float, effect_type: str) -> str:
    """íš¨ê³¼í¬ê¸° ë°°ì§€ ìƒì„±"""
    if effect_type == "cohen_d":
        abs_effect = abs(effect_size) if np.isfinite(effect_size) else 0
        if abs_effect >= 0.8:
            return "ğŸ”¥ **í° íš¨ê³¼**"
        elif abs_effect >= 0.5:
            return "ğŸ“ˆ **ì¤‘ê°„ íš¨ê³¼**"
        elif abs_effect >= 0.2:
            return "ğŸ“Š **ì‘ì€ íš¨ê³¼**"
        else:
            return "ğŸ“‰ **ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” íš¨ê³¼**"
    elif effect_type == "eta_squared":
        if effect_size >= 0.14:
            return "ğŸ”¥ **í° íš¨ê³¼**"
        elif effect_size >= 0.06:
            return "ğŸ“ˆ **ì¤‘ê°„ íš¨ê³¼**"
        elif effect_size >= 0.01:
            return "ğŸ“Š **ì‘ì€ íš¨ê³¼**"
        else:
            return "ğŸ“‰ **ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” íš¨ê³¼**"
    elif effect_type == "r_squared":
        if effect_size >= 0.7:
            return "ğŸ”¥ **ë§¤ìš° ê°•í•œ ì„¤ëª…ë ¥**"
        elif effect_size >= 0.5:
            return "ğŸ“ˆ **ê°•í•œ ì„¤ëª…ë ¥**"
        elif effect_size >= 0.3:
            return "ğŸ“Š **ë³´í†µ ì„¤ëª…ë ¥**"
        else:
            return "ğŸ“‰ **ì•½í•œ ì„¤ëª…ë ¥**"
    return ""

def create_correlation_badge(corr_value: float) -> str:
    """ìƒê´€ê³„ìˆ˜ ë°°ì§€ ìƒì„±"""
    abs_corr = abs(corr_value) if np.isfinite(corr_value) else 0
    if abs_corr >= 0.8:
        return "ğŸ”¥ **ë§¤ìš° ê°•í•œ ìƒê´€**"
    elif abs_corr >= 0.6:
        return "ğŸ“ˆ **ê°•í•œ ìƒê´€**"
    elif abs_corr >= 0.4:
        return "ğŸ“Š **ì¤‘ê°„ ìƒê´€**"
    elif abs_corr >= 0.2:
        return "ğŸ“‰ **ì•½í•œ ìƒê´€**"
    else:
        return "â– **ê±°ì˜ ë¬´ìƒê´€**"

def create_progress_bar(value: float, max_value: float, label: str) -> str:
    """ì§„í–‰ë¥  ë°” ìƒì„±"""
    percentage = (value / max_value) * 100 if max_value > 0 else 0
    percentage = min(100, max(0, percentage))
    
    filled_blocks = int(percentage / 10)
    empty_blocks = 10 - filled_blocks
    
    bar = "â–ˆ" * filled_blocks + "â–‘" * empty_blocks
    return f"{label}: {bar} {percentage:.1f}%"

def display_metric_card(title: str, value: str, delta: str = None, color: str = "normal"):
    """ë©”íŠ¸ë¦­ ì¹´ë“œ í‘œì‹œ"""
    if color == "good":
        st.success(f"**{title}**: {value}" + (f" ({delta})" if delta else ""))
    elif color == "warning":
        st.warning(f"**{title}**: {value}" + (f" ({delta})" if delta else ""))
    elif color == "error":
        st.error(f"**{title}**: {value}" + (f" ({delta})" if delta else ""))
    else:
        st.info(f"**{title}**: {value}" + (f" ({delta})" if delta else ""))

# =============================================================
#  ë¡œë”©/ì „ì²˜ë¦¬
# =============================================================
@st.cache_data(show_spinner=False)
def read_csv_safely(file, sep: Optional[str], encoding: Optional[str]) -> pd.DataFrame:
    kwargs = {}
    if sep:
        kwargs["sep"] = sep
    else:
        kwargs["sep"] = None
        kwargs["engine"] = "python"
    if encoding:
        kwargs["encoding"] = encoding
    try:
        df = pd.read_csv(file, **kwargs)
    except Exception:
        file.seek(0)
        df = pd.read_csv(file, **{**kwargs, "encoding": "cp949"})
    return df

# ì‹œê°í™” ìœ í‹¸

def show_distribution(df: pd.DataFrame, col: str):
    s = df[col].dropna()
    if pd.api.types.is_numeric_dtype(s):
        fig = px.histogram(s, x=col, nbins=30, marginal="box")
        st.plotly_chart(fig, use_container_width=True)
    else:
        vc = s.value_counts().reset_index()
        vc.columns = [col, "count"]
        fig = px.bar(vc, x=col, y="count")
        st.plotly_chart(fig, use_container_width=True)

# =============================================================
#  ë¶„ì„ ì œì•ˆ ë¡œì§
# =============================================================
@dataclass
class Suggestion:
    key: str
    label: str
    desc: str


def suggest_analyses(df: pd.DataFrame, it: InferredTypes, target: Optional[str]) -> List[Suggestion]:
    suggestions: List[Suggestion] = []
    suggestions.append(Suggestion("eda", "ê¸°ì´ˆ EDA ìš”ì•½", "ì»¬ëŸ¼ ìš”ì•½, ê²°ì¸¡, ë¶„í¬, ê°„ë‹¨ ì‹œê°í™”"))
    if len(it.numeric) >= 2:
        suggestions.append(Suggestion("correlation", "ìƒê´€ë¶„ì„ (í”¼ì–´ìŠ¨/ìŠ¤í”¼ì–´ë§Œ)", "ì—°ì†í˜• ë³€ìˆ˜ ê°„ ìƒê´€í–‰ë ¬ê³¼ íˆíŠ¸ë§µ"))
    has_group_compare = any((df[c].nunique(dropna=True) >= 2 and df[c].nunique(dropna=True) <= 10) for c in (it.categorical + it.boolean)) and (len(it.numeric) >= 1)
    if has_group_compare:
        suggestions.append(Suggestion("group_compare", "ê·¸ë£¹ ê°„ í‰ê·  ë¹„êµ (t-ê²€ì •/ANOVA)", "ë²”ì£¼í˜• Ã— ì—°ì†í˜• ì¡°í•©ìœ¼ë¡œ ì§‘ë‹¨ ë¹„êµ"))
        suggestions.append(Suggestion("tukey", "ì‚¬í›„ê²€ì • (Tukey HSD)", "ìœ ì˜í•˜ë©´ ì–´ë–¤ ê·¸ë£¹ë¼ë¦¬ ì°¨ì´ì¸ì§€ í™•ì¸"))
    if len(it.categorical + it.boolean) >= 2:
        suggestions.append(Suggestion("chi_square", "ì¹´ì´ì œê³± ë…ë¦½ì„± ê²€ì •", "ë‘ ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ë…ë¦½ì„± ê²€ì •"))
    if target is not None and target in df.columns:
        y = df[target]
        if pd.api.types.is_numeric_dtype(y) and y.nunique(dropna=True) > 2:
            suggestions.append(Suggestion("linreg", f"âœ… ì„ í˜• íšŒê·€ (íƒ€ê²Ÿ: {target})", f"ì—°ì†í˜• íƒ€ê²Ÿ '{target}' ì˜ˆì¸¡ ëª¨ë¸"))
            suggestions.append(Suggestion("regdiag", "íšŒê·€ ì§„ë‹¨ (VIF/ì”ì°¨)", "ë‹¤ì¤‘ê³µì„ ì„±Â·ì”ì°¨ ì§„ë‹¨"))
            suggestions.append(Suggestion("featimp", f"âœ… íŠ¹ì„± ì¤‘ìš”ë„ (íƒ€ê²Ÿ: {target})", f"'{target}'ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë³€ìˆ˜ ë¶„ì„"))
        else:
            suggestions.append(Suggestion("logreg", f"âœ… ë¡œì§€ìŠ¤í‹± íšŒê·€ (íƒ€ê²Ÿ: {target})", f"ë²”ì£¼í˜• íƒ€ê²Ÿ '{target}' ë¶„ë¥˜ ëª¨ë¸"))
            suggestions.append(Suggestion("featimp", f"âœ… íŠ¹ì„± ì¤‘ìš”ë„ (íƒ€ê²Ÿ: {target})", f"'{target}'ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë³€ìˆ˜ ë¶„ì„"))
    else:
        # íƒ€ê²Ÿì´ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°
        if len(it.numeric) >= 1:
            suggestions.append(Suggestion("linreg", "âš ï¸ ì„ í˜• íšŒê·€ (íƒ€ê²Ÿ ë¯¸ì„¤ì •)", "ì‹¤í–‰ ì‹œ ì—°ì†í˜• íƒ€ê²Ÿì„ ì„ íƒí•´ì•¼ í•¨"))
        binary_cats = [c for c in (it.categorical + it.boolean) if df[c].nunique(dropna=True) == 2]
        if binary_cats:
            suggestions.append(Suggestion("logreg", "âš ï¸ ë¡œì§€ìŠ¤í‹± íšŒê·€ (íƒ€ê²Ÿ ë¯¸ì„¤ì •)", "ì‹¤í–‰ ì‹œ ë²”ì£¼í˜• íƒ€ê²Ÿì„ ì„ íƒí•´ì•¼ í•¨"))
        suggestions.append(Suggestion("featimp", "âš ï¸ íŠ¹ì„± ì¤‘ìš”ë„ (íƒ€ê²Ÿ ë¯¸ì„¤ì •)", "ì‹¤í–‰ ì‹œ íƒ€ê²Ÿì„ ì„ íƒí•´ì•¼ í•¨"))
    if len(it.numeric) >= 2:
        suggestions.append(Suggestion("kmeans", "KMeans êµ°ì§‘í™”", "ì—°ì†í˜• ë³€ìˆ˜ë“¤ë¡œ êµ°ì§‘ íƒìƒ‰ (PCA 2D ì‹œê°í™”)"))
    if it.datetime and it.numeric:
        suggestions.append(Suggestion("timeseries", "ì‹œê³„ì—´ ì¶”ì„¸/ê³„ì ˆì„±", "ë‚ ì§œ ê¸°ì¤€ ì§‘ê³„ í›„ ì¶”ì„¸Â·ê³„ì ˆì„± í™•ì¸"))
    suggestions.append(Suggestion("insights", "ìë™ ì¸ì‚¬ì´íŠ¸", "ìƒê´€Â·ê²°ì¸¡Â·ê·¸ë£¹ì°¨ì´ ë“± í•µì‹¬ ìš”ì•½"))
    suggestions.append(Suggestion("outliers", "ì´ìƒì¹˜ íƒì§€", "IQR/z-scoreÂ·Isolation Forest"))
    return suggestions

# =============================================================
#  ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë¹Œë“œ (ê²°ì¸¡ì¹˜ ì•ˆì „)
# =============================================================

def _build_preprocess(df: pd.DataFrame, x_cols: List[str]) -> Tuple[Pipeline, List[str]]:
    X = df[x_cols]
    num_cols = [c for c in x_cols if pd.api.types.is_numeric_dtype(df[c])]
    cat_cols = [c for c in x_cols if not pd.api.types.is_numeric_dtype(df[c])]
    try:
        ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
    except TypeError:
        ohe = OneHotEncoder(handle_unknown="ignore", sparse=False)

    num_pipe = Pipeline([("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())])
    cat_pipe = Pipeline([("imputer", SimpleImputer(strategy="most_frequent")), ("ohe", ohe)])

    pre = ColumnTransformer(
        transformers=[("num", num_pipe, num_cols), ("cat", cat_pipe, cat_cols)],
        remainder="drop",
        verbose_feature_names_out=False,
    )
    pipe = Pipeline([("pre", pre)])

    X_sample = X.head(2000)
    pre.fit(X_sample)
    try:
        feat_names = pre.get_feature_names_out().tolist()
    except Exception:
        feat_names = []
    return pipe, feat_names

# =============================================================
#  ë¶„ì„ í•¨ìˆ˜ë“¤
# =============================================================

def run_correlation(df: pd.DataFrame, numeric_cols: List[str]):
    st.subheader("ğŸ“ˆ ìƒê´€ë¶„ì„")
    st.caption("í”¼ì–´ìŠ¨: ì„ í˜•ê´€ê³„ r, ìŠ¤í”¼ì–´ë§Œ: ìˆœìœ„ ê¸°ë°˜. |r|ê°€ í´ìˆ˜ë¡ ê°•í•¨.")
    cols = st.multiselect("ë¶„ì„í•  ì—°ì†í˜• ë³€ìˆ˜ ì„ íƒ", options=numeric_cols, default=numeric_cols[:min(5, len(numeric_cols))])
    if len(cols) < 2:
        st.info("ë‘ ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
        return "", None
    corr_p = df[cols].corr(method="pearson")
    corr_s = df[cols].corr(method="spearman")

    # ìƒê´€ë¶„ì„ ìš”ì•½ ëŒ€ì‹œë³´ë“œ
    st.markdown("### ğŸ“Š ìƒê´€ë¶„ì„ ìš”ì•½")
    
    try:
        a = corr_p.abs()
        upper = a.where(np.triu(np.ones(a.shape), k=1).astype(bool))
        pairs = upper.stack().sort_values(ascending=False)
        
        if len(pairs) > 0:
            max_corr = pairs.max()
            strong_pairs = (pairs >= 0.8).sum()
            moderate_pairs = ((pairs >= 0.5) & (pairs < 0.8)).sum()
            weak_pairs = (pairs < 0.3).sum()
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                color = "good" if max_corr < 0.8 else "warning" if max_corr < 0.9 else "error"
                display_metric_card("ìµœëŒ€ ìƒê´€ê³„ìˆ˜", f"{max_corr:.3f}", color=color)
            
            with col2:
                color = "error" if strong_pairs > 0 else "good"
                display_metric_card("ê°•í•œ ìƒê´€ (â‰¥0.8)", f"{strong_pairs}ê°œ", color=color)
            
            with col3:
                display_metric_card("ì¤‘ê°„ ìƒê´€ (0.5-0.8)", f"{moderate_pairs}ê°œ", color="normal")
            
            with col4:
                display_metric_card("ì•½í•œ ìƒê´€ (<0.3)", f"{weak_pairs}ê°œ", color="normal")

    except Exception:
        pass

    st.markdown("**í”¼ì–´ìŠ¨ ìƒê´€í–‰ë ¬**")
    fig1 = px.imshow(corr_p, text_auto=True, aspect="auto", color_continuous_scale="RdBu_r")
    st.plotly_chart(fig1, use_container_width=True)

    st.markdown("**ìŠ¤í”¼ì–´ë§Œ ìƒê´€í–‰ë ¬**")
    fig2 = px.imshow(corr_s, text_auto=True, aspect="auto", color_continuous_scale="RdBu_r")
    st.plotly_chart(fig2, use_container_width=True)

    # ìƒìœ„ ìƒê´€ìŒ ë¶„ì„
    try:
        a = corr_p.abs()
        upper = a.where(np.triu(np.ones(a.shape), k=1).astype(bool))
        pairs = upper.stack().sort_values(ascending=False)
        top = pairs.head(min(5, len(pairs)))
        
        if len(top) > 0:
            st.markdown("### ğŸ” ì£¼ìš” ìƒê´€ê´€ê³„")
            for (i, j), v in top.items():
                corr_val = corr_p.loc[i, j]
                badge = create_correlation_badge(corr_val)
                direction = "ì–‘ì˜ ìƒê´€" if corr_val > 0 else "ìŒì˜ ìƒê´€"
                st.markdown(f"**{i} â†” {j}**: {corr_val:.3f} {badge} ({direction})")
        
        strong_flag = (pairs.max() if len(pairs) > 0 else 0) >= 0.8
        if strong_flag:
            st.warning("âš ï¸ **ì£¼ì˜**: |r|â‰¥0.8ì¸ ê°•í•œ ìƒê´€ìŒì´ ì¡´ì¬í•©ë‹ˆë‹¤. íšŒê·€ë¶„ì„ ì‹œ ë‹¤ì¤‘ê³µì„ ì„±ì„ í™•ì¸í•˜ì„¸ìš”!")
        
        top_lines = [f"- {i}â€“{j}: r={corr_p.loc[i, j]:.2f} ({create_correlation_badge(corr_p.loc[i, j])})" for (i, j), v in top.items()]
    except Exception:
        top_lines, strong_flag = [], False

    md_lines = [
        "### ìƒê´€ë¶„ì„ ê²°ê³¼",
        "í”¼ì–´ìŠ¨/ìŠ¤í”¼ì–´ë§Œ ìƒê´€ì„ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤.",
        "**ì£¼ìš” ìƒê´€ìŒ:**",
    ] + (top_lines if top_lines else ["- (ì¶©ë¶„í•œ ìŒì´ ì—†ìŒ)"])
    if strong_flag:
        md_lines.append("âš ï¸ |r|â‰¥0.8ì¸ ê°•í•œ ìƒê´€ìŒ ì¡´ì¬ â†’ ë‹¤ì¤‘ê³µì„ ì„± ì£¼ì˜")

    return "\n".join(md_lines), {"pearson": corr_p, "spearman": corr_s}


def run_group_compare(df: pd.DataFrame, numeric_cols: List[str], cat_cols: List[str]):
    st.subheader("ğŸ§ª ê·¸ë£¹ ê°„ í‰ê·  ë¹„êµ")
    st.caption("ë‘ ì§‘ë‹¨ì´ë©´ t-ê²€ì •/Mannâ€“Whitney, ì„¸ ì§‘ë‹¨ ì´ìƒì´ë©´ ANOVA/Kruskal-Wallis. p<0.05ë©´ í‰ê·  ì°¨ì´ê°€ ìœ ì˜í•˜ë©°, íš¨ê³¼í¬ê¸° d/Î·Â²ë¡œ í¬ê¸°ë¥¼ í•¨ê»˜ ë´…ë‹ˆë‹¤.")
    num = st.selectbox("ì—°ì†í˜• ë³€ìˆ˜", options=numeric_cols)
    cat = st.selectbox("ê·¸ë£¹(ë²”ì£¼í˜•) ë³€ìˆ˜", options=cat_cols)

    groups = df[[num, cat]].dropna()
    k = groups[cat].nunique()

    auto_nonparam = st.checkbox("ì •ê·œì„± ìœ„ë°˜ ì‹œ ë¹„ëª¨ìˆ˜ ëŒ€ì²´ (Shapiro / Mann-Whitney, Kruskal)", value=True)

    result_md: List[str] = []

    if k == 2:
        levels = groups[cat].dropna().unique()
        g1 = groups[groups[cat] == levels[0]][num].values
        g2 = groups[groups[cat] == levels[1]][num].values
        use_nonparam = False
        if auto_nonparam:
            for g in [g1, g2]:
                sample = g if len(g) <= 5000 else np.random.choice(g, 5000, replace=False)
                try:
                    sh_p = stats.shapiro(sample).pvalue
                    if sh_p < 0.05:
                        use_nonparam = True
                        break
                except Exception:
                    pass
        if use_nonparam:
            stat, p = stats.mannwhitneyu(g1, g2, alternative="two-sided")
            test_name = "Mann-Whitney U"
            eff = np.nan
        else:
            stat, p = stats.ttest_ind(g1, g2, equal_var=False)
            test_name = "ë…ë¦½í‘œë³¸ t-ê²€ì • (Welch)"
            eff = cohen_d(g1, g2)

        # ê²°ê³¼ ëŒ€ì‹œë³´ë“œ
        st.markdown("### ğŸ“Š ê²€ì • ê²°ê³¼ ìš”ì•½")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            sig_badge = create_significance_badge(p)
            st.markdown(f"**í†µê³„ì  ìœ ì˜ì„±**\n\n{sig_badge}")
        
        with col2:
            if not np.isnan(eff):
                effect_badge = create_effect_size_badge(eff, "cohen_d")
                st.markdown(f"**íš¨ê³¼ í¬ê¸°**\n\n{effect_badge}\n\nCohen's d = {eff:.3f}")
        
        with col3:
            mean1, mean2 = np.mean(g1), np.mean(g2)
            diff = abs(mean1 - mean2)
            st.markdown(f"**í‰ê·  ì°¨ì´**\n\n{diff:.3f}\n\n{levels[0]}: {mean1:.2f}\n{levels[1]}: {mean2:.2f}")

        result_md.append(f"**ê²€ì •:** {test_name}\n\n**p-value:** {p:.4g}")
        if not np.isnan(eff):
            result_md.append(f"**íš¨ê³¼í¬ê¸° (Cohen's d):** {eff:.3f}")
        interp = f"**í•´ì„:** {_sig_text(p)}"
        if not np.isnan(eff):
            interp += f" / íš¨ê³¼í¬ê¸°: {eff:.2f} ({_cohen_d_level(eff)})"
        result_md.append(interp)

        fig = px.box(groups, x=cat, y=num, points="all", color=cat)
        fig.update_layout(showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
    else:
        data_groups = [groups[groups[cat] == lv][num].dropna().values for lv in groups[cat].unique()]
        use_nonparam = False
        if auto_nonparam:
            for g in data_groups:
                sample = g if len(g) <= 5000 else np.random.choice(g, 5000, replace=False)
                try:
                    sh_p = stats.shapiro(sample).pvalue
                    if sh_p < 0.05:
                        use_nonparam = True
                        break
                except Exception:
                    pass
        if use_nonparam:
            stat, p = stats.kruskal(*data_groups)
            test_name = "Kruskal-Wallis"
            eta2 = np.nan
        else:
            stat, p = stats.f_oneway(*data_groups)
            test_name = "ì¼ì›ë¶„ì‚°ë¶„ì„ (ANOVA)"
            eta2 = eta_squared_anova(data_groups)

        # ê²°ê³¼ ëŒ€ì‹œë³´ë“œ
        st.markdown("### ğŸ“Š ê²€ì • ê²°ê³¼ ìš”ì•½")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            sig_badge = create_significance_badge(p)
            st.markdown(f"**í†µê³„ì  ìœ ì˜ì„±**\n\n{sig_badge}")
        
        with col2:
            if not np.isnan(eta2):
                effect_badge = create_effect_size_badge(eta2, "eta_squared")
                st.markdown(f"**íš¨ê³¼ í¬ê¸°**\n\n{effect_badge}\n\nEtaÂ² = {eta2:.3f}")
        
        with col3:
            group_means = [np.mean(g) for g in data_groups]
            overall_mean = np.mean([val for group in data_groups for val in group])
            variability = np.std(group_means)
            st.markdown(f"**ê·¸ë£¹ ê°„ ë³€ë™ì„±**\n\n{variability:.3f}\n\nì „ì²´ í‰ê· : {overall_mean:.2f}")

        result_md.append(f"**ê²€ì •:** {test_name}\n\n**p-value:** {p:.4g}")
        if not np.isnan(eta2):
            result_md.append(f"**íš¨ê³¼í¬ê¸° (EtaÂ²):** {eta2:.3f}")
        interp = f"**í•´ì„:** {_sig_text(p)}"
        if not np.isnan(eta2):
            interp += f" / íš¨ê³¼í¬ê¸°: {eta2:.2f} ({_eta2_level(eta2)})"
        result_md.append(interp)

        fig = px.box(groups, x=cat, y=num, points="all", color=cat)
        fig.update_layout(showlegend=False)
        st.plotly_chart(fig, use_container_width=True)

    st.markdown("\n".join(result_md))
    return "\n".join(["### ê·¸ë£¹ ê°„ í‰ê·  ë¹„êµ", *result_md]), None


def run_chi_square(df: pd.DataFrame, cat_cols: List[str]):
    st.subheader("ğŸ”¢ ì¹´ì´ì œê³± ë…ë¦½ì„± ê²€ì •")
    st.caption("ë‘ ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ë…ë¦½ì„± ê²€ì •. p<0.05ë©´ ì—°ê´€ì„±ì´ ìˆìœ¼ë©°, Cramer's V(0~1)ë¡œ ê°•ë„ë¥¼ í•´ì„í•©ë‹ˆë‹¤.")
    c1 = st.selectbox("ë²”ì£¼í˜• ë³€ìˆ˜ 1", options=cat_cols, key="chi1")
    c2 = st.selectbox("ë²”ì£¼í˜• ë³€ìˆ˜ 2", options=[c for c in cat_cols if c != c1], key="chi2")

    sub = df[[c1, c2]].dropna()
    ct = pd.crosstab(sub[c1], sub[c2])
    chi2, p, dof, expected = stats.chi2_contingency(ct)

    st.write("**êµì°¨í‘œ**")
    st.dataframe(ct)

    cv = cramers_v(chi2, n=ct.values.sum(), r=ct.shape[0], c=ct.shape[1])

    md_base = textwrap.dedent(f"""
    ### ì¹´ì´ì œê³± ê²€ì •
    - ì¹´ì´ì œê³± í†µê³„ëŸ‰: {chi2:.3f}
    - ììœ ë„: {dof}
    - p-value: {p:.4g}
    - íš¨ê³¼í¬ê¸° (Cramer's V): {cv:.3f}
    """)
    md = md_base + f"\n**í•´ì„:** {_sig_text(p)} / ì—°ê´€ì„± ê°•ë„: {_cramers_v_level(cv)}"
    st.markdown(md)

    fig = px.imshow(ct, text_auto=True, aspect="auto", labels=dict(x=c2, y=c1, color="ë¹ˆë„"))
    st.plotly_chart(fig, use_container_width=True)

    return md, ct


def run_linear_regression(df: pd.DataFrame, target: str, exclude_cols: List[str]):
    st.subheader("ğŸ“ ì„ í˜• íšŒê·€")
    st.caption("RMSEëŠ” ì˜ˆì¸¡ ì˜¤ì°¨ í¬ê¸°, RÂ²ëŠ” ì„¤ëª…ë ¥(0~1). ë‚®ì€ RMSE, ë†’ì€ RÂ²ê°€ ë°”ëŒì§í•©ë‹ˆë‹¤.")
    candidates = [c for c in df.columns if c != target and c not in exclude_cols]
    x_cols = st.multiselect("ì„¤ëª…ë³€ìˆ˜ ì„ íƒ", options=candidates, default=[c for c in candidates if pd.api.types.is_numeric_dtype(df[c])][:5])
    if not x_cols:
        st.info("ì„¤ëª…ë³€ìˆ˜ë¥¼ í•˜ë‚˜ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
        return "", None

    pre, feat_names = _build_preprocess(df, x_cols)

    data = df[x_cols + [target]].dropna()
    X = data[x_cols]
    y = data[target]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

    reg_pipe = Pipeline([("pre", pre), ("model", LinearRegression())])
    reg_pipe.fit(X_train, y_train)
    y_pred = reg_pipe.predict(X_test)

    rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))
    r2 = float(r2_score(y_test, y_pred))

    # íšŒê·€ë¶„ì„ ê²°ê³¼ ëŒ€ì‹œë³´ë“œ
    st.markdown("### ğŸ“Š íšŒê·€ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        r2_badge = create_effect_size_badge(r2, "r_squared")
        display_metric_card("ê²°ì •ê³„ìˆ˜ (RÂ²)", f"{r2:.3f}", r2_badge.split("**")[1].split("**")[0], 
                          "good" if r2 >= 0.7 else "warning" if r2 >= 0.5 else "error")
    
    with col2:
        try:
            y_sd = float(np.std(y_test.values, ddof=1))
            rel_rmse = rmse / y_sd if y_sd > 0 else np.nan
            if np.isfinite(rel_rmse):
                color = "good" if rel_rmse < 0.5 else "warning" if rel_rmse < 1.0 else "error"
                display_metric_card("ìƒëŒ€ RMSE", f"{rel_rmse:.3f}", "í‘œì¤€í¸ì°¨ ëŒ€ë¹„", color)
        except Exception:
            display_metric_card("RMSE", f"{rmse:.4g}", color="normal")
    
    with col3:
        mae = float(np.mean(np.abs(y_test - y_pred)))
        display_metric_card("í‰ê·  ì ˆëŒ€ ì˜¤ì°¨", f"{mae:.3f}", color="normal")
    
    with col4:
        mape = float(np.mean(np.abs((y_test - y_pred) / y_test)) * 100) if np.all(y_test != 0) else np.nan
        if np.isfinite(mape):
            color = "good" if mape < 10 else "warning" if mape < 20 else "error"
            display_metric_card("MAPE", f"{mape:.1f}%", color=color)

    try:
        y_sd = float(np.std(y_test.values, ddof=1))
        rel_rmse = rmse / y_sd if y_sd > 0 else np.nan
    except Exception:
        rel_rmse = np.nan
    
    interp_lr = f"**í•´ì„:** ì„¤ëª…ë ¥ RÂ²={r2:.2f} ({_r2_level(r2)})"
    if np.isfinite(rel_rmse):
        grade = "ì–‘í˜¸" if rel_rmse < 0.5 else ("ë³´í†µ" if rel_rmse < 1.0 else "ë‚®ìŒ")
        interp_lr += f" / ìƒëŒ€ RMSE(í‘œì¤€í¸ì°¨ ëŒ€ë¹„)={rel_rmse:.2f} â†’ ì˜ˆì¸¡ ì„±ëŠ¥ {grade}"
    st.markdown(interp_lr)

    # ì˜ˆì¸¡ vs ì‹¤ì œ ê·¸ë˜í”„ (ê°œì„ ëœ ì‹œê°í™”)
    df_plot = pd.DataFrame({"ì‹¤ì œ": y_test.values, "ì˜ˆì¸¡": y_pred})
    fig = px.scatter(df_plot, x="ì‹¤ì œ", y="ì˜ˆì¸¡", trendline="ols", 
                     title="ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’ (ëŒ€ê°ì„ ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)")
    
    # ì™„ë²½í•œ ì˜ˆì¸¡ ë¼ì¸ ì¶”ê°€ (y=x)
    min_val, max_val = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())
    fig.add_shape(type="line", x0=min_val, y0=min_val, x1=max_val, y1=max_val,
                  line=dict(color="red", dash="dash"), name="ì™„ë²½í•œ ì˜ˆì¸¡")
    
    st.plotly_chart(fig, use_container_width=True)

    # ì”ì°¨ í”Œë¡¯
    residuals = y_test - y_pred
    fig_resid = px.scatter(x=y_pred, y=residuals, labels={"x": "ì˜ˆì¸¡ê°’", "y": "ì”ì°¨"},
                           title="ì”ì°¨ í”Œë¡¯ (ë¬´ì‘ìœ„ ë¶„í¬ê°€ ì´ìƒì )")
    fig_resid.add_hline(y=0, line_dash="dash", line_color="red")
    st.plotly_chart(fig_resid, use_container_width=True)

    try:
        X_design = pre.transform(X_train)
        X_design = sm.add_constant(X_design)
        ols_model = sm.OLS(y_train, X_design).fit()
        
        with st.expander("ğŸ“‹ ìƒì„¸ í†µê³„ ìš”ì•½ ë³´ê¸°"):
            st.text(ols_model.summary())
    except Exception as e:
        st.info(f"ê³„ìˆ˜ ìš”ì•½ ìƒì„± ì¤‘ ìŠ¤í‚µ: {e}")

    md = textwrap.dedent(f"""
    ### ì„ í˜• íšŒê·€ ê²°ê³¼
    - RMSE: {rmse:.4g}
    - RÂ²: {r2:.4g}
    - í•´ì„: ì„¤ëª…ë ¥ RÂ²={r2:.2f} ({_r2_level(r2)}){f" / ìƒëŒ€ RMSE={rel_rmse:.2f}" if np.isfinite(rel_rmse) else ""}
    """)
    return md, None


def run_logistic_regression(df: pd.DataFrame, target: str, exclude_cols: List[str]):
    st.subheader("ğŸ§­ ë¡œì§€ìŠ¤í‹± íšŒê·€ (ë¶„ë¥˜)")
    st.caption("ì •í™•ë„/ì •ë°€ë„/ì¬í˜„ìœ¨/F1ì€ ë¶„ë¥˜ ì„±ëŠ¥, ROC-AUCëŠ” 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤.")
    candidates = [c for c in df.columns if c != target and c not in exclude_cols]
    x_cols = st.multiselect("ì„¤ëª…ë³€ìˆ˜ ì„ íƒ", options=candidates, default=[c for c in candidates if c != target][:5])

    if not x_cols:
        st.info("ì„¤ëª…ë³€ìˆ˜ë¥¼ í•˜ë‚˜ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
        return "", None

    data = df[x_cols + [target]].dropna()
    X = data[x_cols]
    y = data[target]

    if not pd.api.types.is_numeric_dtype(y):
        y = y.astype("category").cat.codes

    pre, feat_names = _build_preprocess(df, x_cols)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

    clf = Pipeline([("pre", pre), ("model", LogisticRegression(max_iter=500))])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    acc = float(accuracy_score(y_test, y_pred))
    pr, rc, f1, _ = precision_recall_fscore_support(y_test, y_pred, average="weighted")

    st.markdown(f"**Accuracy:** {acc:.4g}  |  **Precision:** {pr:.4g}  |  **Recall:** {rc:.4g}  |  **F1:** {f1:.4g}")

    try:
        if len(np.unique(y_test)) == 2:
            y_proba = clf.predict_proba(X_test)[:, 1]
            auc = float(roc_auc_score(y_test, y_proba))
            fpr, tpr, thr = roc_curve(y_test, y_proba)
            df_roc = pd.DataFrame({"FPR": fpr, "TPR": tpr})
            fig = px.line(df_roc, x="FPR", y="TPR")
            st.plotly_chart(fig, use_container_width=True)
            st.markdown(f"**ROC AUC:** {auc:.4g}")
    except Exception:
        pass

    st.markdown("**ë¶„ë¥˜ ë¦¬í¬íŠ¸**")
    st.text(classification_report(y_test, y_pred))

    try:
        auc_val = None
        if len(np.unique(y_test)) == 2:
            y_proba = clf.predict_proba(X_test)[:, 1]
            auc_val = float(roc_auc_score(y_test, y_proba))
    except Exception:
        auc_val = None
    interp_cls = f"**í•´ì„:** ì •í™•ë„ {acc:.2f}, F1 {f1:.2f}"
    if auc_val is not None and np.isfinite(auc_val):
        interp_cls += f", AUC {auc_val:.2f} ({_auc_level(auc_val)})"
    st.markdown(interp_cls)

    md = textwrap.dedent(f"""
    ### ë¡œì§€ìŠ¤í‹± íšŒê·€ ê²°ê³¼
    - Accuracy: {acc:.4g}
    - Precision/Recall/F1(ê°€ì¤‘): {pr:.4g}/{rc:.4g}/{f1:.4g}
    - í•´ì„: ì •í™•ë„ {acc:.2f}, F1 {f1:.2f}{f", AUC {auc_val:.2f} ({_auc_level(auc_val)})" if auc_val is not None and np.isfinite(auc_val) else ""}
    """)
    return md, None


def run_kmeans(df: pd.DataFrame, numeric_cols: List[str]):
    st.subheader("ğŸ§© KMeans êµ°ì§‘í™”")
    st.caption("ë°ì´í„°ë¥¼ kê°œì˜ êµ°ì§‘ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. ì‹¤ë£¨ì—£ ì§€ìˆ˜ 0.5â†‘ì´ë©´ êµ°ì§‘ ë¶„ë¦¬ê°€ ì–‘í˜¸í•©ë‹ˆë‹¤.")
    cols = st.multiselect("êµ°ì§‘ì— ì‚¬ìš©í•  ì—°ì†í˜• ë³€ìˆ˜", options=numeric_cols, default=numeric_cols[:min(5, len(numeric_cols))])
    if len(cols) < 2:
        st.info("ë‘ ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
        return "", None
    k = st.slider("êµ°ì§‘ ìˆ˜ (k)", min_value=2, max_value=10, value=3, step=1)

    data = df[cols].dropna()
    if len(data) < k:
        st.warning("í‘œë³¸ ìˆ˜ê°€ kë³´ë‹¤ ì ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ k ë˜ëŠ” ë³€ìˆ˜ ì„ íƒ.")
        return "", None

    X = StandardScaler().fit_transform(data.values)
    km = KMeans(n_clusters=k, n_init=10, random_state=42)
    labels = km.fit_predict(X)

    pca = PCA(n_components=2, random_state=42)
    coords = pca.fit_transform(X)
    plot_df = pd.DataFrame({"PC1": coords[:, 0], "PC2": coords[:, 1], "cluster": labels.astype(str)})
    fig = px.scatter(plot_df, x="PC1", y="PC2", color="cluster")
    st.plotly_chart(fig, use_container_width=True)

    try:
        from sklearn.metrics import silhouette_score
        sil = float(silhouette_score(X, labels))
        sil_text = f"{sil:.3f} (0.5â†‘ ì¢‹ìŒ, 0~0.5 ì¤‘ê°„, <0 ë¶„ë¦¬ ì•½í•¨)"
    except Exception:
        sil = np.nan
        sil_text = "ê³„ì‚° ë¶ˆê°€"

    md = textwrap.dedent(f"""
    ### KMeans êµ°ì§‘í™”
    - ì„ íƒ ë³€ìˆ˜ ìˆ˜: {len(cols)}
    - k: {k}
    - ì„¤ëª…ë¶„ì‚° (PCA, 2D): {pca.explained_variance_ratio_.sum():.3f}
    - ì‹¤ë£¨ì—£ ê³„ìˆ˜: {sil_text}
    - í•´ì„: 2D ì„¤ëª…ë¶„ì‚°ì´ ë‚®ìœ¼ë©´ ì‹œê°í™”ì—ì„œ êµ°ì§‘ì´ ê²¹ì³ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ë£¨ì—£ì´ ë†’ì„ìˆ˜ë¡ êµ°ì§‘ ë¶„ë¦¬ê°€ ì„ ëª…í•©ë‹ˆë‹¤.
    """)
    return md, None


def run_time_series(df: pd.DataFrame, datetime_cols: List[str], numeric_cols: List[str]):
    st.subheader("ğŸ•’ ì‹œê³„ì—´ ë¶„ì„")
    st.caption("ì‹œê³„ì—´ì„ ì¶”ì„¸/ê³„ì ˆ/ì”ì°¨ë¡œ ë¶„í•´í•˜ì—¬ íŒ¨í„´ì„ í™•ì¸í•©ë‹ˆë‹¤.")
    dcol = st.selectbox("ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼", options=datetime_cols)
    vcol = st.selectbox("ë¶„ì„í•  ì—°ì†í˜• ë³€ìˆ˜", options=numeric_cols)
    freq = st.selectbox("ë¦¬ìƒ˜í”Œ ì£¼ê¸°", options=["D", "W", "M"], format_func=lambda x: {"D": "ì¼ë³„", "W": "ì£¼ë³„", "M": "ì›”ë³„"}[x])

    df2 = df[[dcol, vcol]].dropna().copy()
    if not np.issubdtype(df2[dcol].dtype, np.datetime64):
        df2[dcol] = pd.to_datetime(df2[dcol], errors="coerce")
    df2 = df2.dropna(subset=[dcol]).sort_values(dcol)
    ts = df2.set_index(dcol)[vcol].astype(float).resample(freq).mean().dropna()

    st.markdown("**ì¶”ì„¸ ê·¸ë˜í”„**")
    fig = px.line(ts.reset_index(), x=dcol, y=vcol)
    st.plotly_chart(fig, use_container_width=True)

    md = ["### ì‹œê³„ì—´ ë¶„ì„"]
    try:
        period = {"D": 7, "W": 52, "M": 12}[freq]
        decomp = seasonal_decompose(ts, period=period, model="additive", extrapolate_trend='freq')
        comp_df = pd.DataFrame({
            "timestamp": ts.index,
            "ê´€ì¸¡ì¹˜": ts.values,
            "ì¶”ì„¸": decomp.trend.values,
            "ê³„ì ˆ": decomp.seasonal.values,
            "ì”ì°¨": decomp.resid.values,
        })
        for comp in ["ì¶”ì„¸", "ê³„ì ˆ", "ì”ì°¨"]:
            fig_c = px.line(comp_df, x="timestamp", y=comp)
            st.plotly_chart(fig_c, use_container_width=True)
        try:
            obs_sd = float(np.nanstd(comp_df["ê´€ì¸¡ì¹˜"]))
            seas_sd = float(np.nanstd(comp_df["ê³„ì ˆ"]))
            ratio = seas_sd / obs_sd if obs_sd > 0 else np.nan
            level = "ëšœë ·" if np.isfinite(ratio) and ratio >= 0.3 else "ì•½í•¨"
            md.append(f"ê³„ì ˆì„± ê°•ë„(í‘œì¤€í¸ì°¨ ë¹„ìœ¨): {ratio:.2f} â†’ {level}")
        except Exception:
            pass
        md.append("ì‹œê³„ì—´ì„ ì¶”ì„¸/ê³„ì ˆ/ì”ì°¨ë¡œ ë¶„í•´í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.info(f"ê³„ì ˆë¶„í•´ ìŠ¤í‚µ: {e}")

    return "\n".join(md), None


def run_insights(df: pd.DataFrame, it: InferredTypes):
    st.subheader("ğŸ’¡ ìë™ ì¸ì‚¬ì´íŠ¸")
    
    # ì „ì²´ ë°ì´í„° ìš”ì•½ ëŒ€ì‹œë³´ë“œ
    st.markdown("### ğŸ“Š ë°ì´í„° ì „ì²´ ìš”ì•½")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_missing = df.isna().sum().sum()
        total_cells = len(df) * len(df.columns)
        missing_pct = (total_missing / total_cells) * 100 if total_cells > 0 else 0
        color = "good" if missing_pct < 5 else "warning" if missing_pct < 15 else "error"
        display_metric_card("ì „ì²´ ê²°ì¸¡ë¥ ", f"{missing_pct:.1f}%", f"{total_missing:,}ê°œ", color)
    
    with col2:
        duplicates = df.duplicated().sum()
        dup_pct = (duplicates / len(df)) * 100 if len(df) > 0 else 0
        color = "good" if dup_pct < 1 else "warning" if dup_pct < 5 else "error"
        display_metric_card("ì¤‘ë³µ í–‰", f"{dup_pct:.1f}%", f"{duplicates}ê°œ", color)
    
    with col3:
        display_metric_card("ë°ì´í„° í¬ê¸°", f"{len(df):,}Ã—{len(df.columns)}", color="normal")
    
    with col4:
        numeric_cols = len(it.numeric)
        cat_cols = len(it.categorical + it.boolean)
        display_metric_card("ë³€ìˆ˜ êµ¬ì„±", f"ìˆ˜ì¹˜:{numeric_cols}, ë²”ì£¼:{cat_cols}", color="normal")

    parts: List[str] = ["### ìë™ ì¸ì‚¬ì´íŠ¸ ìš”ì•½"]

    # ê²°ì¸¡ ìƒìœ„ (ì‹œê°í™” ê°œì„ )
    miss = df.isna().mean().sort_values(ascending=False)
    miss_top = miss.head(10)[miss.head(10) > 0]
    if not miss_top.empty:
        st.markdown("### ğŸ” ê²°ì¸¡ê°’ ë¶„ì„")
        
        # ê²°ì¸¡ë¥  ì°¨íŠ¸
        fig_missing = px.bar(x=miss_top.values, y=miss_top.index, orientation='h',
                           title="ì»¬ëŸ¼ë³„ ê²°ì¸¡ë¥ ", labels={"x": "ê²°ì¸¡ë¥ ", "y": "ì»¬ëŸ¼"})
        fig_missing.update_layout(yaxis={'categoryorder':'total ascending'})
        st.plotly_chart(fig_missing, use_container_width=True)
        
        # ì‹¬ê°í•œ ê²°ì¸¡ ê²½ê³ 
        severe_missing = miss_top[miss_top > 0.5]
        if not severe_missing.empty:
            st.error(f"ğŸš¨ **ì‹¬ê°í•œ ê²°ì¸¡**: {len(severe_missing)}ê°œ ì»¬ëŸ¼ì´ 50% ì´ìƒ ê²°ì¸¡")
            for col, rate in severe_missing.items():
                st.markdown(f"- **{col}**: {rate:.1%} ê²°ì¸¡")
        
        parts.append("**ê²°ì¸¡ ìƒìœ„**\n" + "\n".join([f"- {c}: {v:.1%}" for c, v in miss_top.head(5).items()]))

    # ìƒê´€ ìƒìœ„ (ì‹œê°í™” ê°œì„ )
    if len(it.numeric) >= 2:
        st.markdown("### ğŸ”— ê°•í•œ ìƒê´€ê´€ê³„")
        corr = df[it.numeric].corr(numeric_only=True).abs()
        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
        pairs = upper.stack().sort_values(ascending=False).head(5)
        
        if len(pairs) > 0:
            # ìƒê´€ê´€ê³„ ì°¨íŠ¸
            corr_data = pd.DataFrame({
                'ë³€ìˆ˜ìŒ': [f"{i} â†” {j}" for (i, j), v in pairs.items()],
                'ìƒê´€ê³„ìˆ˜': pairs.values
            })
            
            fig_corr = px.bar(corr_data, x='ìƒê´€ê³„ìˆ˜', y='ë³€ìˆ˜ìŒ', orientation='h',
                            title="ê°•í•œ ìƒê´€ê´€ê³„ Top 5", color='ìƒê´€ê³„ìˆ˜',
                            color_continuous_scale='Reds')
            st.plotly_chart(fig_corr, use_container_width=True)
            
            # ë‹¤ì¤‘ê³µì„ ì„± ê²½ê³ 
            strong_corr = pairs[pairs >= 0.8]
            if len(strong_corr) > 0:
                st.warning(f"âš ï¸ **ë‹¤ì¤‘ê³µì„ ì„± ì£¼ì˜**: {len(strong_corr)}ê°œ ìŒì´ |r|â‰¥0.8")
            
            parts.append("**ê°•í•œ ìƒê´€(ìƒìœ„ 5)**\n" + "\n".join([f"- {i}â€“{j}: |r|={v:.2f}" for (i, j), v in pairs.items()]))

    # ì§‘ë‹¨ì°¨ì´ ìŠ¤ìº” (ì‹œê°í™” ê°œì„ )
    st.markdown("### ğŸ§ª ê·¸ë£¹ ê°„ ìœ ì˜í•œ ì°¨ì´ íƒì§€")
    hits: List[Tuple[float, str]] = []
    cats = (it.categorical + it.boolean)[:5]
    for cat in cats:
        levels = df[cat].dropna().unique()
        if 2 <= len(levels) <= 7:
            for num in it.numeric[:8]:
                sub = df[[num, cat]].dropna()
                if sub[cat].nunique() < 2:
                    continue
                groups = [sub[sub[cat] == lv][num].values for lv in sub[cat].unique()]
                if any(len(g) < 3 for g in groups):
                    continue
                try:
                    _, p = stats.f_oneway(*groups)
                    hits.append((p, f"{num} ~ {cat}"))
                except Exception:
                    continue
    
    hits = sorted(hits, key=lambda x: x[0])[:10]
    if hits:
        # ìœ ì˜ì„± ì°¨íŠ¸
        sig_data = pd.DataFrame({
            'ë³€ìˆ˜ìŒ': [name for p, name in hits],
            'p-value': [p for p, name in hits],
            'ìœ ì˜ì„±': ['ë§¤ìš° ìœ ì˜' if p < 0.001 else 'ìœ ì˜' if p < 0.05 else 'ê²½ê³„ì ' if p < 0.1 else 'ë¹„ìœ ì˜' for p, name in hits]
        })
        
        fig_sig = px.bar(sig_data.head(8), x='p-value', y='ë³€ìˆ˜ìŒ', orientation='h',
                        title="ì§‘ë‹¨ ê°„ ì°¨ì´ ìœ ì˜ì„± (ANOVA p-value)", color='ìœ ì˜ì„±',
                        color_discrete_map={'ë§¤ìš° ìœ ì˜': 'darkgreen', 'ìœ ì˜': 'green', 
                                          'ê²½ê³„ì ': 'orange', 'ë¹„ìœ ì˜': 'red'})
        fig_sig.add_vline(x=0.05, line_dash="dash", line_color="red", 
                         annotation_text="p=0.05 (ìœ ì˜ì„± ê¸°ì¤€)")
        st.plotly_chart(fig_sig, use_container_width=True)
        
        # ë§¤ìš° ìœ ì˜í•œ ì°¨ì´ ê°•ì¡°
        very_sig = [name for p, name in hits if p < 0.001]
        if very_sig:
            st.success(f"ğŸ¯ **ë§¤ìš° ìœ ì˜í•œ ì°¨ì´ ë°œê²¬**: {len(very_sig)}ê°œ")
            for name in very_sig[:5]:
                st.markdown(f"- {name}")
        
        parts.append("**ì§‘ë‹¨ ê°„ ì°¨ì´ ê°ì§€(ANOVA, ìƒìœ„ 5)**\n" + "\n".join([f"- {name}: {create_significance_badge(p)}" for p, name in hits[:5]]))

    # ì´ìƒì¹˜ ê°„ë‹¨ íƒì§€
    if it.numeric:
        st.markdown("### ğŸš¨ ì´ìƒì¹˜ ê°„ë‹¨ íƒì§€")
        outlier_summary = []
        
        for col in it.numeric[:5]:  # ìƒìœ„ 5ê°œ ìˆ˜ì¹˜ ì»¬ëŸ¼ë§Œ
            s = df[col].dropna()
            if len(s) > 0:
                q1, q3 = np.percentile(s, [25, 75])
                iqr = q3 - q1
                outliers = ((s < q1 - 1.5 * iqr) | (s > q3 + 1.5 * iqr)).sum()
                outlier_pct = (outliers / len(s)) * 100
                outlier_summary.append({'ì»¬ëŸ¼': col, 'ì´ìƒì¹˜_ê°œìˆ˜': outliers, 'ì´ìƒì¹˜_ë¹„ìœ¨': outlier_pct})
        
        if outlier_summary:
            outlier_df = pd.DataFrame(outlier_summary)
            fig_outlier = px.bar(outlier_df, x='ì»¬ëŸ¼', y='ì´ìƒì¹˜_ë¹„ìœ¨',
                               title="ì»¬ëŸ¼ë³„ ì´ìƒì¹˜ ë¹„ìœ¨ (IQR ê¸°ì¤€)")
            st.plotly_chart(fig_outlier, use_container_width=True)

    return "\n".join(parts), None


def run_feature_importance(df: pd.DataFrame, target_hint: Optional[str]):
    st.subheader("ğŸŒŸ íŠ¹ì„± ì¤‘ìš”ë„")
    st.caption("í¼ë®¤í…Œì´ì…˜ ì¤‘ìš”ë„(ê²€ì¦ì…‹)ì„ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì‹¤íŒ¨ ì‹œ ëœë¤í¬ë ˆìŠ¤íŠ¸ ì¤‘ìš”ë„ë¡œ í´ë°±í•©ë‹ˆë‹¤. ê°’ì´ í´ìˆ˜ë¡ íƒ€ê²Ÿì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í½ë‹ˆë‹¤.")
    if target_hint is None or target_hint not in df.columns:
        tgt = st.selectbox("íƒ€ê²Ÿ ì„ íƒ", options=df.columns.tolist())
    else:
        tgt = target_hint
        st.info(f"íƒ€ê²Ÿ: **{tgt}**")

    X_cols = [c for c in df.columns if c != tgt]
    X = df[X_cols]
    y = df[tgt]

    problem = "reg" if pd.api.types.is_numeric_dtype(y) and y.nunique(dropna=True) > 2 else "clf"

    pre, _ = _build_preprocess(df, X_cols)

    strat = None
    if problem == "clf":
        vc = pd.Series(y).value_counts()
        if len(vc) < 2:
            st.warning("íƒ€ê²Ÿ í´ë˜ìŠ¤ê°€ 1ê°œë¼ ë¶„ë¥˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "### íŠ¹ì„± ì¤‘ìš”ë„\níƒ€ê²Ÿ í´ë˜ìŠ¤ê°€ 1ê°œë¼ ê³„ì‚° ë¶ˆê°€", None
        if vc.min() >= 2:
            strat = y
        else:
            st.info("íƒ€ê²Ÿì˜ ì¼ë¶€ í´ë˜ìŠ¤ í‘œë³¸ ìˆ˜ê°€ 2 ë¯¸ë§Œì´ë¼ ì¸µí™” ì—†ì´ ë¶„í• í•©ë‹ˆë‹¤. (í¬ì†Œ í´ë˜ìŠ¤ ì£¼ì˜)")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=strat)

    if problem == "reg":
        model = Pipeline([("pre", pre), ("rf", RandomForestRegressor(n_estimators=300, random_state=42))])
    else:
        model = Pipeline([("pre", pre), ("rf", RandomForestClassifier(n_estimators=300, random_state=42))])
    model.fit(X_train, y_train)

    imp_df = pd.DataFrame()
    try:
        imp = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)
        try:
            feat_names = model.named_steps["pre"].get_feature_names_out().tolist()
        except Exception:
            feat_names = [f"f{i}" for i in range(len(imp.importances_mean))]
        imp_df = pd.DataFrame({"feature": feat_names, "importance": imp.importances_mean})
    except Exception:
        pass

    if imp_df.empty:
        try:
            rf = model.named_steps.get("rf")
            fi = getattr(rf, "feature_importances_", None)
            if fi is not None:
                try:
                    feat_names = model.named_steps["pre"].get_feature_names_out().tolist()
                except Exception:
                    feat_names = [f"f{i}" for i in range(len(fi))]
                imp_df = pd.DataFrame({"feature": feat_names, "importance": fi})
        except Exception:
            pass

    if not imp_df.empty:
        imp_df = imp_df.sort_values("importance", ascending=False).head(15)
        fig = px.bar(imp_df.iloc[:15][::-1], x="importance", y="feature", orientation="h")
        st.plotly_chart(fig, use_container_width=True)
        md = "### íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„)\n" + "\n".join([f"- {r.feature}: {r.importance:.3f}" for r in imp_df.itertuples(index=False)])
    else:
        st.info("ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë°ì´í„°/ë²„ì „ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ)")
        md = "### íŠ¹ì„± ì¤‘ìš”ë„\nê³„ì‚° ë¶ˆê°€"
    return md, None


def run_tukey(df: pd.DataFrame, it: InferredTypes):
    st.subheader("ğŸ§ª ì‚¬í›„ê²€ì • (Tukey HSD)")
    num = st.selectbox("ì—°ì†í˜• ë³€ìˆ˜", options=it.numeric)
    cat = st.selectbox("ê·¸ë£¹(ë²”ì£¼í˜•) ë³€ìˆ˜", options=(it.categorical + it.boolean))
    sub = df[[num, cat]].dropna()
    if sub[cat].nunique() < 3:
        st.info("Tukey HSDëŠ” 3ê°œ ì´ìƒì˜ ê·¸ë£¹ì—ì„œ ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤.")
        return "", None

    res = pairwise_tukeyhsd(sub[num].values, sub[cat].astype(str).values)

    # í‘œ ë³€í™˜
    try:
        tbl = pd.DataFrame(res._results_table.data[1:], columns=res._results_table.data[0])
        for c in ["meandiff", "p-adj", "lower", "upper"]:
            if c in tbl.columns:
                tbl[c] = pd.to_numeric(tbl[c], errors="coerce")
        if "reject" in tbl.columns:
            tbl["reject"] = tbl["reject"].astype(str)
    except Exception:
        tbl = pd.DataFrame()

    n_pairs = len(tbl)
    sig_tbl = tbl[tbl.get("p-adj", pd.Series(dtype=float)) < 0.05] if not tbl.empty else pd.DataFrame()
    sig_cnt = int(len(sig_tbl)) if not sig_tbl.empty else 0

    MAX_SHOW = 200
    if not tbl.empty:
        sorted_tbl = tbl.sort_values("p-adj", na_position="last") if "p-adj" in tbl.columns else tbl
        st.markdown(f"**ì´ ë¹„êµìŒ:** {n_pairs:,}  |  **ìœ ì˜(p<0.05):** {sig_cnt:,} ({(sig_cnt/n_pairs):.1% if n_pairs>0 else 0})")
        top_preview = sorted_tbl.head(20)
        st.markdown("**ìƒìœ„ 20ê°œ(ê°€ì¥ ìœ ì˜í•œ ìˆœ)**")
        st.dataframe(top_preview, use_container_width=True)

        csv_bytes = sorted_tbl.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ì „ì²´ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ", data=csv_bytes, file_name="tukey_results.csv", mime="text/csv")

        if n_pairs <= MAX_SHOW:
            with st.expander("ì „ì²´ ê²°ê³¼ í‘œ ë³´ê¸°"):
                st.dataframe(sorted_tbl, use_container_width=True)
    else:
        st.text(res.summary())

    means = sub.groupby(cat)[num].agg(["mean", "count", "std"]).reset_index()
    means["se"] = means["std"] / np.sqrt(means["count"].clip(lower=1))
    fig = px.bar(means, x=cat, y="mean", error_y="se")
    st.plotly_chart(fig, use_container_width=True)

    md_lines = [
        "### Tukey HSD ê²°ê³¼",
        f"ì´ ë¹„êµìŒ: {n_pairs:,}",
        f"ìœ ì˜í•œ ìŒ(p<0.05): {sig_cnt:,}",
    ]
    if sig_cnt > 0 and not sig_tbl.empty:
        top_lines = [f"- {r.group1} vs {r.group2}: p={r['p-adj']:.3g}" for _, r in sig_tbl.sort_values("p-adj").head(10).iterrows()]
        md_lines.append("**ìƒìœ„ 10ê°œ ìœ ì˜ ìŒ**")
        md_lines.extend(top_lines)
    md = "\n".join(md_lines)
    return md, None


def run_outliers(df: pd.DataFrame, it: InferredTypes):
    st.subheader("ğŸš¨ ì´ìƒì¹˜ íƒì§€")
    st.caption("IQR ê²½ê³„(1.5Ã—IQR)ë¡œ ë‹¨ë³€ëŸ‰ ì´ìƒì¹˜ë¥¼ ë³´ê³ , ì˜µì…˜ìœ¼ë¡œ Isolation Forestë¡œ ë‹¤ë³€ëŸ‰ ì´ìƒì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.")
    cols = st.multiselect("ê²€ì‚¬í•  ì—°ì†í˜• ë³€ìˆ˜", options=it.numeric, default=it.numeric[:min(5, len(it.numeric))])
    if not cols:
        return "", None

    summary = []
    for c in cols:
        s = df[c].dropna().astype(float)
        if s.empty:
            continue
        q1, q3 = np.percentile(s, [25, 75])
        iqr = q3 - q1
        lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr
        mask = (df[c] < lo) | (df[c] > hi)
        count = int(mask.sum())
        summary.append({"ì»¬ëŸ¼": c, "IQR ë²”ìœ„": f"[{lo:.3g}, {hi:.3g}]", "ì´ìƒì¹˜ ìˆ˜": count})
    if summary:
        st.markdown("**IQR ê¸°ì¤€ ìš”ì•½**")
        st.dataframe(pd.DataFrame(summary))

    if st.checkbox("Isolation Forestë¡œ ë‹¤ë³€ëŸ‰ ì´ìƒì¹˜ íƒì§€") and len(cols) >= 2:
        sub = df[cols].dropna()
        if len(sub) >= 10:
            X = StandardScaler().fit_transform(sub.values)
            iso = IsolationForest(random_state=42, contamination="auto")
            labels = iso.fit_predict(X)
            out_idx = sub.index[labels] == -1
            st.markdown(f"**Isolation Forest ì´ìƒì¹˜ ìˆ˜:** {(out_idx).sum()}")
            if (out_idx).sum() > 0:
                st.dataframe(df.loc[sub.index[out_idx]].head(50))
        else:
            st.info("í‘œë³¸ ìˆ˜ê°€ ì ì–´ ë‹¤ë³€ëŸ‰ ì´ìƒì¹˜ íƒì§€ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")

    md = """### ì´ìƒì¹˜ íƒì§€
- IQRë¡œ ì»¬ëŸ¼ë³„ ê²½ê³„ì™€ ê°œìˆ˜ë¥¼ ìš”ì•½í–ˆìŠµë‹ˆë‹¤. ë‹¤ë³€ëŸ‰ ì´ìƒì¹˜ëŠ” Isolation Forest ì˜µì…˜ìœ¼ë¡œ í™•ì¸í•˜ì„¸ìš”."""
    return md, None


def run_regression_diagnostics(df: pd.DataFrame, target_hint: Optional[str]):
    st.subheader("ğŸ©º íšŒê·€ ì§„ë‹¨")
    if target_hint is None or target_hint not in df.columns or not pd.api.types.is_numeric_dtype(df[target_hint]):
        st.info("ì—°ì†í˜• íƒ€ê²Ÿì„ ì‚¬ì´ë“œë°”ì—ì„œ ì§€ì •í•˜ë©´ íšŒê·€ ì§„ë‹¨ì„ ì œê³µí•©ë‹ˆë‹¤.")
        return "", None

    tgt = target_hint
    num_cols = [c for c in df.columns if c != tgt and pd.api.types.is_numeric_dtype(df[c])]
    if len(num_cols) < 2:
        st.info("ì§„ë‹¨ì„ ìœ„í•œ ì—°ì†í˜• ì„¤ëª…ë³€ìˆ˜ê°€ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.")
        return "", None

    data = df[num_cols + [tgt]].dropna()
    X = data[num_cols]
    y = data[tgt]

    X_const = sm.add_constant(X)
    model = sm.OLS(y, X_const).fit()

    vif_df = pd.DataFrame({
        "ë³€ìˆ˜": ["const"] + num_cols,
        "VIF": [np.nan] + [variance_inflation_factor(X_const.values, i + 1) for i in range(len(num_cols))],
    })
    st.markdown("**VIF (ë‹¤ì¤‘ê³µì„ ì„±)**")
    st.dataframe(vif_df)

    fitted = model.fittedvalues
    resid = model.resid

    fig1 = px.scatter(x=fitted, y=resid, labels={"x": "ì í•©ê°’", "y": "ì”ì°¨"})
    st.plotly_chart(fig1, use_container_width=True)

    qq = sm.ProbPlot(resid)
    theo = qq.theoretical_quantiles
    samp = np.sort(resid)
    fig2 = px.scatter(x=theo, y=samp, labels={"x": "ì´ë¡  ë¶„ìœ„ìˆ˜", "y": "í‘œë³¸ ë¶„ìœ„ìˆ˜"})
    st.plotly_chart(fig2, use_container_width=True)

    md = """### íšŒê·€ ì§„ë‹¨
- VIFê°€ 10ì„ í¬ê²Œ ë„˜ëŠ” ë³€ìˆ˜ëŠ” ë‹¤ì¤‘ê³µì„ ì„± ê°€ëŠ¥ì„± ìˆìŒ
- ì”ì°¨ê°€ ë“±ë¶„ì‚°/ì •ê·œì„±ì„ í¬ê²Œ ë²—ì–´ë‚˜ë©´ ëª¨ë¸ ìˆ˜ì • í•„ìš”"""
    return md, None

# =============================================================
#  ë©”ì¸ UI
# =============================================================

st.title("ğŸ“Š CSV í†µê³„ë¶„ì„")
st.markdown("""
<div style="padding: 1rem; background-color: #f0f2f6; border-radius: 0.5rem; margin-bottom: 1rem;">
    <h4>ğŸ¯ ì´ ë„êµ¬ì˜ íŠ¹ì§•</h4>
    <ul>
        <li><strong>ìë™ ë¶„ì„ ì œì•ˆ</strong>: ë°ì´í„° íƒ€ì…ì„ ìë™ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì í•©í•œ í†µê³„ë¶„ì„ì„ ì¶”ì²œí•©ë‹ˆë‹¤</li>
        <li><strong>ì§ê´€ì ì¸ ê²°ê³¼</strong>: ìƒ‰ìƒ ì½”ë”©ê³¼ ì‹œê°ì  ì§€í‘œë¡œ ê²°ê³¼ë¥¼ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤</li>
        <li><strong>ì´ˆë³´ì ì¹œí™”ì </strong>: ë³µì¡í•œ í†µê³„ ìš©ì–´ë¥¼ ì‰¬ìš´ ë§ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤</li>
        <li><strong>ì¢…í•© ë³´ê³ ì„œ</strong>: ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì •ë¦¬ëœ ë³´ê³ ì„œë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤</li>
    </ul>
</div>
""", unsafe_allow_html=True)

# ë„ì›€ë§ í™•ì¥ ê°€ëŠ¥í•œ ì„¹ì…˜
with st.expander("â“ ì‚¬ìš©ë²• ë° ë„ì›€ë§"):
    st.markdown("""
    ### ğŸ“‹ ê¸°ë³¸ ì‚¬ìš©ë²•
    1. **CSV íŒŒì¼ ì—…ë¡œë“œ**: ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ CSV íŒŒì¼ì„ ì„ íƒí•˜ê³  ì—…ë¡œë“œí•˜ì„¸ìš”
    2. **íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •** (ì„ íƒì‚¬í•­): ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ ë³€ìˆ˜ê°€ ìˆë‹¤ë©´ íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”
    3. **ë¶„ì„ ì„ íƒ**: ì¶”ì²œëœ ë¶„ì„ ëª©ë¡ì—ì„œ ì›í•˜ëŠ” ë¶„ì„ì„ ì„ íƒí•˜ì„¸ìš”
    4. **ê²°ê³¼ í™•ì¸**: ìƒ‰ìƒ ì½”ë”©ëœ ê²°ê³¼ì™€ í•´ì„ì„ í™•ì¸í•˜ì„¸ìš”
    5. **ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ**: ë¶„ì„ì´ ì™„ë£Œë˜ë©´ ì „ì²´ ë³´ê³ ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”
    
    ### ğŸ¨ ê²°ê³¼ í•´ì„ ê°€ì´ë“œ
    - **ğŸŸ© ì´ˆë¡ìƒ‰**: ì¢‹ì€ ê²°ê³¼ë‚˜ ê¶Œì¥ ìƒíƒœ
    - **ğŸŸ¡ ë…¸ë€ìƒ‰**: ì£¼ì˜ê°€ í•„ìš”í•œ ìƒíƒœ  
    - **ğŸ”´ ë¹¨ê°„ìƒ‰**: ë¬¸ì œê°€ ìˆê±°ë‚˜ ê°œì„ ì´ í•„ìš”í•œ ìƒíƒœ
    - **ğŸ“Š ì§„í–‰ë¥  ë°”**: ê°ì¢… ì§€í‘œì˜ ìˆ˜ì¤€ì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ
    
    ### ğŸ’¡ ë¶„ì„ ìœ í˜•ë³„ ê°€ì´ë“œ
    - **ê¸°ì´ˆ EDA**: ë°ì´í„°ì˜ ê¸°ë³¸ íŠ¹ì„±ê³¼ ë¶„í¬ë¥¼ íŒŒì•…í•  ë•Œ ì‚¬ìš©
    - **ìƒê´€ë¶„ì„**: ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ í™•ì¸í•  ë•Œ ì‚¬ìš©
    - **ê·¸ë£¹ ë¹„êµ**: ë²”ì£¼ë³„ë¡œ í‰ê· ì— ì°¨ì´ê°€ ìˆëŠ”ì§€ í™•ì¸í•  ë•Œ ì‚¬ìš©
    - **íšŒê·€ë¶„ì„**: íŠ¹ì • ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©
    - **ìë™ ì¸ì‚¬ì´íŠ¸**: ë°ì´í„°ì—ì„œ ìë™ìœ¼ë¡œ íŒ¨í„´ì„ ì°¾ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©
    """)

with st.sidebar:
    st.header("1) CSV ì—…ë¡œë“œ")
    with st.form("load_form", clear_on_submit=False):
        up = st.file_uploader("CSV íŒŒì¼ ì„ íƒ", type=["csv"], key="up")
        st.markdown("**ê³ ê¸‰ ì„¤ì •**")
        col1, col2 = st.columns(2)
        sep = col1.text_input("êµ¬ë¶„ì (ë¹„ì›Œë‘ë©´ ìë™)", value=st.session_state.get("sep", ""))
        enc = col2.text_input("ì¸ì½”ë”© (ë¹„ì›Œë‘ë©´ ìë™)", value=st.session_state.get("enc", ""))

        st.divider()
        st.header("2) íƒ€ê²Ÿ ë³€ìˆ˜ (ì„ íƒ)")
        target_hint_in = st.text_input("ì˜ˆ: price, outcome ë“± (ì—†ìœ¼ë©´ ë¹„ì›Œë‘ê¸°)", value=st.session_state.get("target_hint", ""))
        submitted = st.form_submit_button("ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°", type="primary")

    if submitted:
        if up is None:
            st.warning("CSV íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.")
        else:
            try:
                df_tmp = read_csv_safely(up, sep or None, enc or None)
                st.session_state["df"] = df_tmp
                st.session_state["sep"] = sep
                st.session_state["enc"] = enc
                st.session_state["target_hint"] = (target_hint_in or None)
                st.session_state["file_name"] = up.name
                st.success(f"íŒŒì¼ ë¡œë“œ ì™„ë£Œ! (í–‰ {len(df_tmp)}, ì—´ {len(df_tmp.columns)})")
            except Exception as e:
                st.error(f"CSVë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")

    if "df" in st.session_state:
        st.caption(f"ë¶ˆëŸ¬ì˜¨ íŒŒì¼: {st.session_state.get('file_name','(ì´ë¦„ ì—†ìŒ)')}")
        if st.button("ğŸ” ë‹¤ë¥¸ íŒŒì¼ë¡œ êµì²´/ì˜µì…˜ ë³€ê²½", use_container_width=True):
            for k in ["df", "file_name"]:
                if k in st.session_state:
                    del st.session_state[k]

if "df" not in st.session_state:
    st.info("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ CSV íŒŒì¼ê³¼ ì˜µì…˜ì„ ì„ íƒí•œ ë’¤ **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    st.stop()

# ì´í›„ë¶€í„°ëŠ” ì„¸ì…˜ì˜ dfì™€ íƒ€ê¹ƒ íŒíŠ¸ë¥¼ ì‚¬ìš©
df = st.session_state["df"]
target_hint = st.session_state.get("target_hint")

st.success(f"íŒŒì¼ ë¡œë“œ ì™„ë£Œ! (í–‰ {len(df)}, ì—´ {len(df.columns)})")

# íƒ€ì… ì¶”ë¡ 
inferred = infer_types(df)

# ê°œìš”
with st.expander("ë°ì´í„° ê°œìš”", expanded=True):
    st.write("**ì»¬ëŸ¼ íƒ€ì… ì¶”ë¡ **")
    t1, t2, t3, t4 = st.columns(4)
    t1.metric("ì—°ì†í˜•", len(inferred.numeric))
    t2.metric("ë²”ì£¼í˜•", len(inferred.categorical))
    t3.metric("ë¶ˆë¦¬ì–¸", len(inferred.boolean))
    t4.metric("ë‚ ì§œ/ì‹œê°„", len(inferred.datetime))

    st.dataframe(df.head())

    with st.expander("ì»¬ëŸ¼ë³„ ìš”ì•½ ë³´ê¸°"):
        try:
            desc = df.describe(include="all", datetime_is_numeric=True).T
        except TypeError:
            desc = df.describe(include="all").T
        st.dataframe(desc)

# ì œì•ˆ ìƒì„±
suggestions = suggest_analyses(df, inferred, target_hint)

st.header("ğŸ¯ ê°€ëŠ¥í•œ ë¶„ì„ ì œì•ˆ")

# íƒ€ê²Ÿ ì„¤ì • ìƒíƒœì— ë”°ë¥¸ ì•ˆë‚´
if target_hint and target_hint in df.columns:
    st.success(f"âœ… **íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •ë¨**: {target_hint}")
    st.info("ğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ì˜ˆì¸¡ ëª¨ë¸ë§ê³¼ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
else:
    st.warning("âš ï¸ **íƒ€ê²Ÿ ë³€ìˆ˜ ë¯¸ì„¤ì •**: ì˜ˆì¸¡ ë¶„ì„ì„ ì›í•œë‹¤ë©´ ì‚¬ì´ë“œë°”ì—ì„œ íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    with st.expander("ğŸ’¡ íƒ€ê²Ÿ ë³€ìˆ˜ë€?"):
        st.markdown("""
        **íƒ€ê²Ÿ ë³€ìˆ˜**ëŠ” ì˜ˆì¸¡í•˜ê±°ë‚˜ ì„¤ëª…í•˜ê³  ì‹¶ì€ ë³€ìˆ˜ì…ë‹ˆë‹¤.
        
        **ì˜ˆì‹œ:**
        - ê³ ê° ë°ì´í„°ì—ì„œ **'ë§Œì¡±ë„'**ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ë‹¤ë©´ â†’ íƒ€ê²Ÿ: ë§Œì¡±ë„
        - ì£¼íƒ ë°ì´í„°ì—ì„œ **'ê°€ê²©'**ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ë‹¤ë©´ â†’ íƒ€ê²Ÿ: ê°€ê²©  
        - ì˜ë£Œ ë°ì´í„°ì—ì„œ **'ì§ˆë³‘ì—¬ë¶€'**ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¶ë‹¤ë©´ â†’ íƒ€ê²Ÿ: ì§ˆë³‘ì—¬ë¶€
        
        **íƒ€ê²Ÿ ì„¤ì • ì‹œ ì¶”ê°€ ë¶„ì„:**
        - âœ… íšŒê·€/ë¶„ë¥˜ ì˜ˆì¸¡ ëª¨ë¸
        - âœ… íŠ¹ì„± ì¤‘ìš”ë„ (ì–´ë–¤ ë³€ìˆ˜ê°€ íƒ€ê²Ÿì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€)
        - âœ… íšŒê·€ ì§„ë‹¨ (ëª¨ë¸ì˜ í’ˆì§ˆ ê²€ì¦)
        """)

st.divider()

# ë¶„ì„ ì œì•ˆ ëª©ë¡ (íƒ€ê²Ÿ ì„¤ì • ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í‘œì‹œ)
for s in suggestions:
    if s.label.startswith("âœ…"):
        st.success(f"**{s.label}** â€” {s.desc}")
    elif s.label.startswith("âš ï¸"):
        st.warning(f"**{s.label}** â€” {s.desc}")
    else:
        st.markdown(f"- **{s.label}** â€” {s.desc}")

st.divider()

# ì‹¤í–‰í•  ë¶„ì„ ì„ íƒ
labels_map = {s.label: s.key for s in suggestions}
# ê¸°ë³¸ ì„ íƒ: ì´ˆë³´ì ì¹œí™”(EDA/ìƒê´€/ìë™ ì¸ì‚¬ì´íŠ¸/ì´ìƒì¹˜, +íŠ¹ì„±ì¤‘ìš”ë„(ìˆìœ¼ë©´))
default_keys = {"eda", "correlation", "insights", "outliers"}
if any(s.key == "featimp" for s in suggestions):
    default_keys.add("featimp")
default_labels = [s.label for s in suggestions if s.key in default_keys]
chosen_labels = st.multiselect(
    "ì‹¤í–‰í•  ë¶„ì„ì„ ì„ íƒí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
    options=list(labels_map.keys()),
    default=default_labels,
)

# ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
if "run" not in st.session_state:
    st.session_state.run = False
if "chosen_labels" not in st.session_state:
    st.session_state.chosen_labels = []

col_run1, col_run2 = st.columns([1, 1])
with col_run1:
    run_click = st.button("ë¶„ì„ ì‹¤í–‰", type="primary")
with col_run2:
    reset_click = st.button("ê²°ê³¼ ì´ˆê¸°í™”")

if run_click:
    st.session_state.run = True
    st.session_state.chosen_labels = chosen_labels

if reset_click:
    st.session_state.run = False
    st.session_state.chosen_labels = []

report_parts: List[str] = []

if st.session_state.run and st.session_state.chosen_labels:
    st.header("ğŸ¯ ë¶„ì„ ê²°ê³¼")

    # ì „ì²´ ê²°ê³¼ ìš”ì•½ ì„¹ì…˜
    st.markdown("### ğŸŒŸ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìš”ì•½")
    
    # ì¸ì‚¬ì´íŠ¸ ì¹´ë“œë“¤ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ
    insight_container = st.container()
    key_insights = []
    
    active_labels = [lbl for lbl in st.session_state.chosen_labels if lbl in labels_map]
    
    # ê°„ë‹¨í•œ ì‚¬ì „ ë¶„ì„ìœ¼ë¡œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
    with insight_container:
        col1, col2, col3 = st.columns(3)
        
        # ë°ì´í„° í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸
        with col1:
            total_missing = df.isna().sum().sum()
            total_cells = len(df) * len(df.columns)
            missing_pct = (total_missing / total_cells) * 100 if total_cells > 0 else 0
            
            if missing_pct < 5:
                st.success("âœ… **ë°ì´í„° í’ˆì§ˆ ìš°ìˆ˜**\n\nê²°ì¸¡ë¥ ì´ 5% ë¯¸ë§Œìœ¼ë¡œ ë¶„ì„ì— ì í•©í•©ë‹ˆë‹¤.")
            elif missing_pct < 15:
                st.warning("âš ï¸ **ë°ì´í„° í’ˆì§ˆ ë³´í†µ**\n\nì¼ë¶€ ê²°ì¸¡ê°’ì´ ìˆì–´ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            else:
                st.error("ğŸš¨ **ë°ì´í„° í’ˆì§ˆ ì£¼ì˜**\n\nê²°ì¸¡ë¥ ì´ ë†’ì•„ ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ë³€ìˆ˜ êµ¬ì„± ì¸ì‚¬ì´íŠ¸
        with col2:
            num_vars = len(inferred.numeric)
            cat_vars = len(inferred.categorical + inferred.boolean)
            
            if num_vars >= 3 and cat_vars >= 2:
                st.success("ğŸ¯ **ë¶„ì„ ê°€ëŠ¥ì„± ë†’ìŒ**\n\në‹¤ì–‘í•œ í†µê³„ë¶„ì„ì´ ê°€ëŠ¥í•œ ë°ì´í„°ì…ë‹ˆë‹¤.")
            elif num_vars >= 2 or cat_vars >= 2:
                st.info("ğŸ“Š **ê¸°ë³¸ ë¶„ì„ ê°€ëŠ¥**\n\nê¸°ë³¸ì ì¸ í†µê³„ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            else:
                st.warning("ğŸ“ˆ **ì œí•œì  ë¶„ì„**\n\në³€ìˆ˜ê°€ ì ì–´ ë¶„ì„ì´ ì œí•œì ì…ë‹ˆë‹¤.")
        
        # ìƒ˜í”Œ í¬ê¸° ì¸ì‚¬ì´íŠ¸
        with col3:
            sample_size = len(df)
            
            if sample_size >= 1000:
                st.success("ğŸ“Š **ì¶©ë¶„í•œ ìƒ˜í”Œ**\n\ní†µê³„ì  ê²€ì •ì— ì í•©í•œ í¬ê¸°ì…ë‹ˆë‹¤.")
            elif sample_size >= 100:
                st.info("ğŸ“ˆ **ì ì ˆí•œ ìƒ˜í”Œ**\n\nê¸°ë³¸ ë¶„ì„ì— ì í•©í•©ë‹ˆë‹¤.")
            else:
                st.warning("âš ï¸ **ì‘ì€ ìƒ˜í”Œ**\n\nê²°ê³¼ í•´ì„ ì‹œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    st.divider()

    for lbl in active_labels:
        key = labels_map[lbl]
        st.divider()
        if key == "eda":
            st.subheader("ğŸ” ê¸°ì´ˆ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)")
            
            # ì»¬ëŸ¼ ì„ íƒê³¼ ê¸°ë³¸ ì •ë³´
            col = st.selectbox("ë¶„ì„í•  ì»¬ëŸ¼ ì„ íƒ", options=df.columns.tolist(), key=f"eda_{lbl}")
            s = df[col]
            
            # ê¸°ë³¸ í†µê³„ ëŒ€ì‹œë³´ë“œ
            st.markdown("### ğŸ“Š ê¸°ë³¸ í†µê³„ ìš”ì•½")
            col1, col2, col3, col4 = st.columns(4)
            
            miss = float(s.isna().mean())
            with col1:
                color = "good" if miss < 0.05 else "warning" if miss < 0.2 else "error"
                display_metric_card("ê²°ì¸¡ë¥ ", f"{miss:.1%}", f"{s.isna().sum()}ê°œ", color)
            
            with col2:
                display_metric_card("ì´ ê°œìˆ˜", f"{len(s):,}", color="normal")
            
            with col3:
                unique_count = s.nunique(dropna=True)
                unique_pct = (unique_count / len(s.dropna())) * 100 if len(s.dropna()) > 0 else 0
                display_metric_card("ê³ ìœ ê°’", f"{unique_count:,}", f"{unique_pct:.1f}%", "normal")
            
            with col4:
                data_type = "ìˆ˜ì¹˜í˜•" if pd.api.types.is_numeric_dtype(s) else "ë²”ì£¼í˜•"
                display_metric_card("ë°ì´í„° íƒ€ì…", data_type, color="normal")
            
            show_distribution(df, col)
            
            if pd.api.types.is_numeric_dtype(s):
                # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìƒì„¸ ë¶„ì„
                st.markdown("### ğŸ“ˆ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìƒì„¸ ë¶„ì„")
                
                s_clean = s.dropna()
                if len(s_clean) > 0:
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**ğŸ“Š ì¤‘ì‹¬ê²½í–¥ ì§€í‘œ**")
                        mean_val = float(s_clean.mean())
                        median_val = float(s_clean.median())
                        mode_val = float(s_clean.mode().iloc[0]) if len(s_clean.mode()) > 0 else np.nan
                        
                        st.metric("í‰ê·  (Mean)", f"{mean_val:.3f}")
                        st.metric("ì¤‘ì•™ê°’ (Median)", f"{median_val:.3f}")
                        if np.isfinite(mode_val):
                            st.metric("ìµœë¹ˆê°’ (Mode)", f"{mode_val:.3f}")
                    
                    with col2:
                        st.markdown("**ğŸ“ ë¶„ì‚° ì§€í‘œ**")
                        std_val = float(s_clean.std())
                        var_val = float(s_clean.var())
                        cv_val = (std_val / mean_val) * 100 if mean_val != 0 else np.nan
                        
                        st.metric("í‘œì¤€í¸ì°¨", f"{std_val:.3f}")
                        st.metric("ë¶„ì‚°", f"{var_val:.3f}")
                        if np.isfinite(cv_val):
                            st.metric("ë³€ë™ê³„ìˆ˜ (CV)", f"{cv_val:.1f}%")
                    
                    # ë¶„í¬ ëª¨ì–‘ ë¶„ì„
                    skew = float(s_clean.skew())
                    kurt = float(s_clean.kurtosis())
                    
                    st.markdown("### ğŸ“ ë¶„í¬ì˜ ëª¨ì–‘")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        if abs(skew) < 0.5:
                            skew_interp = "ëŒ€ì¹­ì  ë¶„í¬ âš–ï¸"
                            skew_color = "good"
                        elif skew > 0:
                            skew_interp = "ì˜¤ë¥¸ìª½ ê¼¬ë¦¬ê°€ ê¸´ ë¶„í¬ â†—ï¸"
                            skew_color = "warning"
                        else:
                            skew_interp = "ì™¼ìª½ ê¼¬ë¦¬ê°€ ê¸´ ë¶„í¬ â†–ï¸"
                            skew_color = "warning"
                        
                        display_metric_card("ì™œë„ (Skewness)", f"{skew:.3f}", skew_interp, skew_color)
                    
                    with col2:
                        if abs(kurt) < 0.5:
                            kurt_interp = "ì •ê·œë¶„í¬ì™€ ë¹„ìŠ·í•œ ë¾°ì¡±í•¨ ğŸ“Š"
                            kurt_color = "good"
                        elif kurt > 0:
                            kurt_interp = "ì •ê·œë¶„í¬ë³´ë‹¤ ë¾°ì¡±í•¨ ğŸ“ˆ"
                            kurt_color = "warning"
                        else:
                            kurt_interp = "ì •ê·œë¶„í¬ë³´ë‹¤ í‰í‰í•¨ ğŸ“‰"
                            kurt_color = "warning"
                        
                        display_metric_card("ì²¨ë„ (Kurtosis)", f"{kurt:.3f}", kurt_interp, kurt_color)
                    
                    # ì´ìƒì¹˜ íƒì§€
                    q1, q3 = np.percentile(s_clean, [25, 75])
                    iqr = q3 - q1
                    outliers = ((s_clean < q1 - 1.5 * iqr) | (s_clean > q3 + 1.5 * iqr)).sum()
                    outlier_pct = (outliers / len(s_clean)) * 100
                    
                    if outliers > 0:
                        color = "warning" if outlier_pct < 5 else "error"
                        st.markdown("### ğŸš¨ ì´ìƒì¹˜ íƒì§€")
                        display_metric_card("ì´ìƒì¹˜ ê°œìˆ˜", f"{outliers}ê°œ", f"{outlier_pct:.1f}%", color)
                
                md = textwrap.dedent(f"""
                ### ê¸°ì´ˆ EDA ìš”ì•½ - {col} (ìˆ˜ì¹˜í˜•)
                - ê²°ì¸¡ ë¹„ìœ¨: {miss:.1%}
                - í‰ê· : {mean_val:.3f}, ì¤‘ì•™ê°’: {median_val:.3f}
                - ì™œë„: {skew:.3f} ({skew_interp})
                - ì´ìƒì¹˜: {outliers}ê°œ ({outlier_pct:.1f}%)
                """)
            else:
                # ë²”ì£¼í˜• ë³€ìˆ˜ ìƒì„¸ ë¶„ì„
                st.markdown("### ğŸ“‹ ë²”ì£¼í˜• ë³€ìˆ˜ ìƒì„¸ ë¶„ì„")
                
                nunq = int(s.nunique(dropna=True))
                value_counts = s.value_counts().head(10)
                
                # ë¹ˆë„ ì°¨íŠ¸
                if len(value_counts) > 0:
                    fig_freq = px.bar(x=value_counts.index.astype(str), y=value_counts.values,
                                    title=f"ìƒìœ„ {min(10, len(value_counts))}ê°œ ê°’ì˜ ë¹ˆë„")
                    st.plotly_chart(fig_freq, use_container_width=True)
                
                # ë¶„í¬ ê· ë“±ì„± ë¶„ì„
                total_count = s.count()
                if total_count > 0:
                    entropy = -sum((p := count/total_count) * np.log2(p) for count in value_counts if count > 0)
                    max_entropy = np.log2(min(nunq, len(value_counts)))
                    balance_ratio = entropy / max_entropy if max_entropy > 0 else 0
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        color = "good" if balance_ratio > 0.8 else "warning" if balance_ratio > 0.5 else "error"
                        display_metric_card("ë¶„í¬ ê· ë“±ì„±", f"{balance_ratio:.1%}", 
                                          "ê· ë“±í• ìˆ˜ë¡ ì¢‹ìŒ" if balance_ratio > 0.8 else "ë¶ˆê· ë“±í•¨", color)
                    
                    with col2:
                        most_common_pct = (value_counts.iloc[0] / total_count) * 100
                        color = "warning" if most_common_pct > 50 else "error" if most_common_pct > 80 else "good"
                        display_metric_card("ìµœë¹ˆê°’ ë¹„ìœ¨", f"{most_common_pct:.1f}%", 
                                          f"'{value_counts.index[0]}'", color)
                
                top3 = value_counts.head(3)
                top_lines = "\n".join([f"  â€¢ {idx}: {val}ê°œ ({val/total_count:.1%})" for idx, val in top3.items()])
                
                md = textwrap.dedent(f"""
                ### ê¸°ì´ˆ EDA ìš”ì•½ - {col} (ë²”ì£¼í˜•)
                - ê³ ìœ ê°’: {nunq}ê°œ
                - ê²°ì¸¡ ë¹„ìœ¨: {miss:.1%}
                - ë¶„í¬ ê· ë“±ì„±: {balance_ratio:.1%}
                - ìµœë¹ˆê°’: '{value_counts.index[0]}' ({most_common_pct:.1f}%)

                **ìƒìœ„ 3ê°œ ê°’**
                {top_lines}
                """)
            
            # ì‹¤ìš©ì ì¸ ì¡°ì–¸ ì¶”ê°€
            st.markdown("### ğŸ’¡ ë°ì´í„° í’ˆì§ˆ ì¡°ì–¸")
            advice = []
            
            if miss > 0.2:
                advice.append("âš ï¸ **ê²°ì¸¡ë¥ ì´ ë†’ìŠµë‹ˆë‹¤** - ê²°ì¸¡ê°’ ì²˜ë¦¬ ì „ëµì„ ê³ ë ¤í•˜ì„¸ìš”")
            elif miss > 0.05:
                advice.append("ğŸ“ **ê²°ì¸¡ê°’ì´ ì¼ë¶€ ìˆìŠµë‹ˆë‹¤** - ë¶„ì„ ì‹œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            if pd.api.types.is_numeric_dtype(s):
                if outlier_pct > 5:
                    advice.append("ğŸš¨ **ì´ìƒì¹˜ê°€ ë§ìŠµë‹ˆë‹¤** - ì´ìƒì¹˜ ì œê±°ë‚˜ ë³€í™˜ì„ ê³ ë ¤í•˜ì„¸ìš”")
                if abs(skew) > 1:
                    advice.append("ğŸ“ **ë¶„í¬ê°€ ì‹¬í•˜ê²Œ ì¹˜ìš°ì³ ìˆìŠµë‹ˆë‹¤** - ë¡œê·¸ë³€í™˜ ë“±ì„ ê³ ë ¤í•˜ì„¸ìš”")
            else:
                if nunq == len(s.dropna()):
                    advice.append("ğŸ” **ëª¨ë“  ê°’ì´ ê³ ìœ í•©ë‹ˆë‹¤** - ì‹ë³„ì ì»¬ëŸ¼ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤")
                elif balance_ratio < 0.5:
                    advice.append("âš–ï¸ **ë¶„í¬ê°€ ë¶ˆê· ë“±í•©ë‹ˆë‹¤** - í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ê³ ë ¤í•˜ì„¸ìš”")
            
            if advice:
                for adv in advice:
                    st.markdown(adv)
            else:
                st.success("âœ… **ë°ì´í„° í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤!**")
            
            st.markdown(md)
            report_parts.append(md)

        elif key == "correlation":
            md, _ = run_correlation(df, inferred.numeric)
            st.markdown(md)
            report_parts.append(md)

        elif key == "group_compare":
            md, _ = run_group_compare(df, inferred.numeric, inferred.categorical + inferred.boolean)
            report_parts.append(md)

        elif key == "chi_square":
            md, _ = run_chi_square(df, inferred.categorical + inferred.boolean)
            report_parts.append(md)

        elif key == "linreg":
            if target_hint and target_hint in df.columns and pd.api.types.is_numeric_dtype(df[target_hint]):
                # íƒ€ê²Ÿì´ ë¯¸ë¦¬ ì„¤ì •ëœ ê²½ìš°
                st.info(f"ğŸ¯ **ì„¤ì •ëœ íƒ€ê²Ÿ**: {target_hint}")
                tgt = target_hint
            else:
                # íƒ€ê²Ÿì´ ì—†ëŠ” ê²½ìš° ì‚¬ìš©ìê°€ ì„ íƒ
                st.warning("âš ï¸ íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¶„ì„í•  íƒ€ê²Ÿì„ ì„ íƒí•˜ì„¸ìš”.")
                tgt = st.selectbox("ğŸ“Š ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ ì—°ì†í˜• ë³€ìˆ˜ ì„ íƒ", options=inferred.numeric)
                if tgt:
                    st.info(f"ğŸ’¡ **ì¶”ì²œ**: ë‹¤ìŒë²ˆì—ëŠ” ì‚¬ì´ë“œë°”ì—ì„œ '{tgt}'ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ë¯¸ë¦¬ ì„¤ì •í•˜ì„¸ìš”!")
            
            md, _ = run_linear_regression(df, tgt, exclude_cols=[tgt])
            report_parts.append(md)

        elif key == "logreg":
            if target_hint and target_hint in df.columns and not pd.api.types.is_numeric_dtype(df[target_hint]):
                # íƒ€ê²Ÿì´ ë¯¸ë¦¬ ì„¤ì •ëœ ê²½ìš°
                st.info(f"ğŸ¯ **ì„¤ì •ëœ íƒ€ê²Ÿ**: {target_hint}")
                tgt = target_hint
            else:
                # íƒ€ê²Ÿì´ ì—†ëŠ” ê²½ìš° ì‚¬ìš©ìê°€ ì„ íƒ
                st.warning("âš ï¸ íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¶„ì„í•  íƒ€ê²Ÿì„ ì„ íƒí•˜ì„¸ìš”.")
                tgt = st.selectbox("ğŸ“Š ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ ë²”ì£¼í˜• ë³€ìˆ˜ ì„ íƒ", options=inferred.categorical + inferred.boolean)
                if tgt:
                    st.info(f"ğŸ’¡ **ì¶”ì²œ**: ë‹¤ìŒë²ˆì—ëŠ” ì‚¬ì´ë“œë°”ì—ì„œ '{tgt}'ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ë¯¸ë¦¬ ì„¤ì •í•˜ì„¸ìš”!")
            
            md, _ = run_logistic_regression(df, tgt, exclude_cols=[tgt])
            report_parts.append(md)

        elif key == "featimp":
            if target_hint and target_hint in df.columns:
                st.info(f"ğŸ¯ **ì„¤ì •ëœ íƒ€ê²Ÿ**: {target_hint}")
                md, _ = run_feature_importance(df, target_hint)
            else:
                st.warning("âš ï¸ íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¤‘ìš”ë„ë¥¼ ë¶„ì„í•  íƒ€ê²Ÿì„ ì„ íƒí•˜ì„¸ìš”.")
                md, _ = run_feature_importance(df, None)  # í•¨ìˆ˜ ë‚´ì—ì„œ ë‹¤ì‹œ ì„ íƒí•˜ê²Œ ë¨
                
            st.markdown(md)
            report_parts.append(md)

        elif key == "kmeans":
            md, _ = run_kmeans(df, inferred.numeric)
            st.markdown(md)
            report_parts.append(md)

        elif key == "timeseries":
            if not inferred.datetime:
                st.info("ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ì´ ì—†ì–´ ì‹œê³„ì—´ ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.")
            else:
                md, _ = run_time_series(df, inferred.datetime, inferred.numeric)
                st.markdown(md)
                report_parts.append(md)

        elif key == "tukey":
            md, _ = run_tukey(df, inferred)
            st.markdown(md)
            report_parts.append(md)

        elif key == "outliers":
            md, _ = run_outliers(df, inferred)
            st.markdown(md)
            report_parts.append(md)

        elif key == "insights":
            md, _ = run_insights(df, inferred)
            st.markdown(md)
            report_parts.append(md)

    st.divider()

    # ---------- ìµœì¢… ìš”ì•½ ë° ê¶Œê³ ì‚¬í•­ ----------
    st.markdown("### ğŸ¯ ë¶„ì„ ì™„ë£Œ ìš”ì•½")
    
    summary_cols = st.columns(3)
    with summary_cols[0]:
        st.metric("ì™„ë£Œëœ ë¶„ì„", f"{len(active_labels)}ê°œ", "ì„ íƒí•œ ë¶„ì„ ëª¨ë‘ ì™„ë£Œ")
    
    with summary_cols[1]:
        analyzed_rows = len(df)
        st.metric("ë¶„ì„ëœ ë°ì´í„°", f"{analyzed_rows:,}í–‰", f"{len(df.columns)}ê°œ ì»¬ëŸ¼")
    
    with summary_cols[2]:
        if "insights" in [labels_map[lbl] for lbl in active_labels]:
            st.metric("í•µì‹¬ ì¸ì‚¬ì´íŠ¸", "í¬í•¨ë¨", "ìë™ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì™„ë£Œ")
        else:
            st.metric("ì¶”ê°€ ë¶„ì„", "ê°€ëŠ¥", "ë” ë§ì€ ì¸ì‚¬ì´íŠ¸ ë°œê²¬ ê°€ëŠ¥")

    # ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­
    st.markdown("### ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­")
    
    next_steps = []
    completed_keys = {labels_map[lbl] for lbl in active_labels}
    
    if "correlation" in completed_keys and "regdiag" not in completed_keys:
        next_steps.append("ğŸ” **íšŒê·€ ì§„ë‹¨ ì‹¤í–‰** - ìƒê´€ê´€ê³„ë¥¼ ë°œê²¬í–ˆë‹¤ë©´ íšŒê·€ ì§„ë‹¨ìœ¼ë¡œ ë‹¤ì¤‘ê³µì„ ì„±ì„ í™•ì¸í•´ë³´ì„¸ìš”")
    
    if "group_compare" in completed_keys and "tukey" not in completed_keys:
        next_steps.append("ğŸ“Š **ì‚¬í›„ê²€ì • ì‹¤í–‰** - ê·¸ë£¹ ê°„ ì°¨ì´ê°€ ìœ ì˜í•˜ë‹¤ë©´ Tukey ê²€ì •ìœ¼ë¡œ ì–´ë–¤ ê·¸ë£¹ë¼ë¦¬ ë‹¤ë¥¸ì§€ í™•ì¸í•´ë³´ì„¸ìš”")
    
    if len(inferred.numeric) >= 2 and "kmeans" not in completed_keys:
        next_steps.append("ğŸ§© **êµ°ì§‘ë¶„ì„ ê³ ë ¤** - ë°ì´í„°ì— ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ì°¾ê¸° ìœ„í•´ K-means ë¶„ì„ì„ í•´ë³´ì„¸ìš”")
    
    if target_hint and "featimp" not in completed_keys:
        next_steps.append("ğŸŒŸ **íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„** - íƒ€ê²Ÿ ë³€ìˆ˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì£¼ìš” ë³€ìˆ˜ë“¤ì„ í™•ì¸í•´ë³´ì„¸ìš”")
    
    if "outliers" not in completed_keys:
        next_steps.append("ğŸš¨ **ì´ìƒì¹˜ íƒì§€** - ë°ì´í„° í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ ì´ìƒì¹˜ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”")

    if next_steps:
        for step in next_steps:
            st.markdown(step)
    else:
        st.success("ğŸ‰ **ë¶„ì„ ì™„ë£Œ!** ëª¨ë“  ì£¼ìš” ë¶„ì„ì„ ë§ˆì³¤ìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì„ ì§„í–‰í•˜ì„¸ìš”.")

    # ---------- ë¦¬í¬íŠ¸ ëª¨ë“œ ---------- 
    st.divider()
    st.markdown("### ğŸ“‹ ë¶„ì„ ë³´ê³ ì„œ")
    
    # ë³´ê³ ì„œ í—¤ë” ì¶”ê°€
    report_header = f"""
# ğŸ“Š ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ

**ë¶„ì„ ì¼ì‹œ**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
**ë°ì´í„° í¬ê¸°**: {len(df):,}í–‰ Ã— {len(df.columns)}ê°œ ì»¬ëŸ¼
**ì‹¤í–‰ëœ ë¶„ì„**: {', '.join(active_labels)}

---

"""
    
    report_md = report_header + "\n\n".join(report_parts)
    
    # ë³´ê³ ì„œ ë¯¸ë¦¬ë³´ê¸°ì™€ ë‹¤ìš´ë¡œë“œ
    col1, col2 = st.columns([3, 1])
    
    with col1:
        with st.expander("ğŸ“ ì „ì²´ ë³´ê³ ì„œ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
            st.markdown(report_md)
    
    with col2:
        st.download_button(
            "ğŸ“¥ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ\n(Markdown)",
            data=report_md.encode("utf-8-sig"),
            file_name=f"analysis_report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.md",
            mime="text/markdown",
            use_container_width=True
        )
        
        # ê°„ë‹¨í•œ í†µê³„ ìš”ì•½ë„ ë‹¤ìš´ë¡œë“œ ì˜µì…˜ ì œê³µ
        if not df.empty:
            summary_stats = df.describe(include='all').to_csv()
            st.download_button(
                "ğŸ“Š ê¸°ì´ˆí†µê³„ ë‹¤ìš´ë¡œë“œ\n(CSV)",
                data=summary_stats.encode("utf-8-sig"),
                file_name=f"summary_stats_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv",
                use_container_width=True
            )
